{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParallelNotes.jl #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome! Appologies to Amrit for the notebook form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\chev8\\.julia\\dev\\ParallelNotes`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") # activate this project -- that's all this does\n",
    "\n",
    "# this will call the packages and APIs we need!!\n",
    "using LinearAlgebra, CUDA, Hwloc, BenchmarkTools, JuMP, Ipopt, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will teach us something abour parallel computing in\t[Julia](https://julialang.org/benchmarks/). We won't jump right into GPU programming, however, since parallelization on the CPU should always be explored before GPU programming is exploited. **Crawl, walk, run, FLY!**\n",
    "\n",
    "So, let's start by asking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Should I parallelize my code?\"**\n",
    "\t\n",
    "* **Does performance matter && does your problem naturally parallelize??**\n",
    "    * *No*: Probably no need/not possible. **END**.\n",
    "    * *Yes*: Let's see what we can do!\n",
    "        * **Do you need massive parallelization || can functions run on GPU?**\n",
    "            * *Either No*: Just use CPU threads. **END**.\n",
    "            * *Both Yes*: Use the GPU!\n",
    "                * **Are you willing to sacrifice a little speed for a lot of conveniencesimplicity?**\n",
    "                    * *Yes*: Use Cu/ROC/one/Mtl Arrays + LinAlg APIs. **END**.\n",
    "                    * *No*: Use GPU Kernels.\n",
    "                        * **Is your code Pure NVIDIA?**\n",
    "                            * *Yes*: Just use the @cuda kernal macro.\n",
    "                            * *No*: Use KernelAbstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Julia specific tutorial, but its premise is universal. If you are coming from C++, good news: [\"writing kernels in Julia is very similar to writing kernels in CUDA C/C++\"](https://cuda.juliagpu.org/stable/development/kernel/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a quick tl;dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to write GPU accelerated code, here is Sam's suggestion:\n",
    "1. write highly optimized CPU code (minimize run-time memory allocations, ensure the code is \"type stable\", exploit optimized BLAS routines, etc), **then**\n",
    "2. parallelize the CPU code, testing for speedup and correctness, **then**\n",
    "3. move the the GPU, exploiting GPU arrays or custom kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to write optimized GPU code you need to\n",
    "1. have an impeccable benchmark on the CPU first\n",
    "2. be obsessed with timing statistics and memory allocation\n",
    "3. realize that it will take a lot of tinkering before it's optimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three levels of GPU utilization: applications (e.g., Flux, or DiffEqs), arrays, and kernels. This is summarized by Time Besard in the following figures:\n",
    "![app](./figs/apps.png)\n",
    "![arrays](./figs/arrays.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. CPU Parallelization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try some CPU parallelization exercises. Let's see how many CPU threads we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Available CPU Threads: 20. Physical Threads: 14\n",
      "Julia is only using this many threads: 6\n"
     ]
    }
   ],
   "source": [
    "println(\"Total Available CPU Threads: \", Sys.CPU_THREADS, \". Physical Threads: \", num_physical_cores())\n",
    "println(\"Julia is only using this many threads: \", Threads.nthreads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without telling Julia to multi-thread, it will perform opperations on a single thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for ii in 1:48\n",
    "\tprintln(Threads.threadid())\n",
    "\tsleep(0.1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the ```Threads.@threads``` macro to parallelize the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "1\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "Threads.@threads for ii in 1:48\n",
    "\tprintln(Threads.threadid())\n",
    "\tsleep(0.1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1: CPU Parallelized Matrix Vector Products ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to compute $m$ matrix vector products $y=Ax$, i.e., $y\\in{\\mathbb R}^{n \\times m}$, $A\\in{\\mathbb R}^{n \\times n}$, and $n\\in{\\mathbb R}^{n \\times m}$. Without parallelization, you might just compute the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  $y_1 = Ax_1$\n",
    "*  $y_2 = Ax_2$\n",
    "*  ...\n",
    "*  $y_m = Ax_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvp_cpu_serial (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick note: Matrix{Float64} == Array{Float64,2}\n",
    "function mvp_cpu_serial(A::Matrix{Float64}, x::Array{Float64,2}, y::Array{Float64,2})\n",
    "\tfor ii in 1:size(x,2)\n",
    "\t\ty[:,ii] .= A*x[:,ii] # note! \"mul!\" would be slightly faster\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.060863 seconds (300 allocations: 787.500 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0024876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_mvps = 50;\n",
    "A = randn(1000,1000);\n",
    "x = randn(1000,num_mvps);\n",
    "y = randn(1000,num_mvps);\n",
    "@time mvp_cpu_serial(A,x,y) # quick and dirty\n",
    "time_serial = @belapsed mvp_cpu_serial($A,$x,$y) # much more percise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the CPU to parallelize! We can use the `Threads@threads` macros to tell Julia to **parallelize** the operation, i.e., to chop up the taks and assign it to available CPU threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvp_cpu_parallel (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mvp_cpu_parallel(A::Matrix{Float64}, x::Array{Float64,2}, y::Array{Float64,2})\n",
    "\tThreads.@threads for ii in 1:size(x,2)\n",
    "\t\ty[:,ii] .= A*x[:,ii] # note! \"mul!\" would be slightly faster\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.073708 seconds (334 allocations: 791.391 KiB)\n",
      "Parallel vs serial speedup: 1.351075385618075\n"
     ]
    }
   ],
   "source": [
    "num_mvps = 50;\n",
    "A = randn(1000,1000);\n",
    "x = randn(1000,num_mvps);\n",
    "y = randn(1000,num_mvps);\n",
    "@time mvp_cpu_parallel(A,x,y) # quick and dirty\n",
    "time_parallel = @belapsed mvp_cpu_parallel($A,$x,$y) # much more percise!\n",
    "\n",
    "println(\"Parallel vs serial speedup: \", time_serial/time_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2: CPU Parallelized Optimization ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have $m$ optimization problems we need to solve. For example, in the GO competition, I wanted to parallelize across power flows solutions in space, and generator feasibility over time (startup/shutdown/ramping constraints), as shown in Fig. 2 of [this paper.](https://arxiv.org/pdf/2310.06650): ![parallel](./figs/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pose $m$ parallelizable optimization problems as the followin $\\min\\;x_{i}^{T}M_{i}x_{i},\\;{\\rm s.t.}\\,A_{i}x_{i}\\le b_{i}$ (where $M_i$ is PSD, don't worry :). That is, we want to solve\n",
    "*  $\\min\\;x_{1}^{T}M_{1}x_{1},\\;{\\rm s.t.}\\,A_{1}x_{1}\\le b_{1}$\n",
    "*  $\\min\\;x_{2}^{T}M_{2}x_{2},\\;{\\rm s.t.}\\,A_{2}x_{2}\\le b_{2}$\n",
    "* ...\n",
    "*  $\\min\\;x_{m}^{T}M_{m}x_{m},\\;{\\rm s.t.}\\,A_{m}x_{m}\\le b_{m}$,\n",
    "\n",
    "where each of these problems can be solved in parallel. Let's solve these in series, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve_opts (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function opt(M,A,b)\n",
    "    nv = length(b)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, x[1:nv])\n",
    "    @constraint(model, A*x .<= b)\n",
    "    @objective(model, Min, x'*M*x)\n",
    "    optimize!(model)\n",
    "    return objective_value(model)\n",
    "end\n",
    "\n",
    "function solve_opts(M,A,b;serial=true)\n",
    "    n_opt = length(M)\n",
    "    if serial == true\n",
    "        for ii in 1:n_opt\n",
    "            opt(M[ii],A[ii],b[ii])\n",
    "        end\n",
    "    else\n",
    "        Threads.@threads for ii in 1:n_opt\n",
    "            opt(M[ii],A[ii],b[ii])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the parallel optimization problem, and test the serial vs parallel implementations (see ```run_opt.jl``` -- the notebook implementation was giving me some trouble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. GPU Parallelization via GPU Arrays ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU programming is actually very eary! Via GPUArrays (which you never actually need to use, since it's a backend), Julia users can define arrays which live on GPU device memory. A wide set of operations applied to these arrays will automatically run directly on the GPU. As summarized [here](https://enccs.github.io/julia-for-hpc/GPU/), we use the following:\n",
    "* CUDA.jl for NVIDIA GPUs\n",
    "* AMDGPU.jl for AMD GPUs\n",
    "* oneAPI.jl for Intel GPUs\n",
    "* Metal.jl for Apple M-series GPUs\n",
    "\n",
    "Moving from a CPU-based array to a GPU-based array, we simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Float64, 1, CUDA.DeviceMemory}:\n",
       "  0.4140075600845548\n",
       "  0.11872512384693543\n",
       "  0.5660972614454215\n",
       " -1.1920732122395217\n",
       " -0.09467233272673888\n",
       "  0.5731141408489481\n",
       " -0.07274485039141936\n",
       "  0.1907454874378951\n",
       " -1.979013543844737\n",
       " -1.2757699199763282\n",
       "  ⋮\n",
       "  0.5912011643287084\n",
       "  0.1091594185730021\n",
       "  1.14368157076373\n",
       " -0.6870781725353733\n",
       "  1.0083289600936167\n",
       "  0.015855086655486086\n",
       " -1.8326480478861091\n",
       " -2.8369153166226653\n",
       "  1.404038657911186"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cpu = randn(100)\n",
    "x_gpu = CuArray(x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Float64, 1, CUDA.DeviceMemory}:\n",
       " 0.17140225980716625\n",
       " 0.014095655032470155\n",
       " 0.3204661094160059\n",
       " 1.4210385433390518\n",
       " 0.008962850583922353\n",
       " 0.3284598184410279\n",
       " 0.005291813258469984\n",
       " 0.0363838409779202\n",
       " 3.9164946067209043\n",
       " 1.6275888887164067\n",
       " ⋮\n",
       " 0.34951881670362045\n",
       " 0.011915778663195875\n",
       " 1.3080075353045928\n",
       " 0.4720764151745482\n",
       " 1.0167272917634744\n",
       " 0.00025138377285297296\n",
       " 3.3585988674207665\n",
       " 8.048088513688278\n",
       " 1.9713245529090446"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = x_gpu.^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, you just ran your **first GPU calculations** in Julia! It was that easy! So, how much computation can you push to the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfGPUMemoryError",
     "evalue": "Out of GPU memory trying to allocate 74.506 GiB\nEffective GPU memory usage: 23.77% (1.901 GiB/7.996 GiB)\nMemory pool usage: 770.883 MiB (832.000 MiB reserved)\n",
     "output_type": "error",
     "traceback": [
      "Out of GPU memory trying to allocate 74.506 GiB\n",
      "Effective GPU memory usage: 23.77% (1.901 GiB/7.996 GiB)\n",
      "Memory pool usage: 770.883 MiB (832.000 MiB reserved)\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] _pool_alloc\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:660 [inlined]\n",
      " [2] macro expansion\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:617 [inlined]\n",
      " [3] macro expansion\n",
      "   @ .\\timing.jl:421 [inlined]\n",
      " [4] pool_alloc\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:616 [inlined]\n",
      " [5] CuArray{Int64, 2, CUDA.DeviceMemory}(::UndefInitializer, dims::Tuple{Int64, Int64})\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:74\n",
      " [6] CuArray\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:128 [inlined]\n",
      " [7] (CuArray{Int64})(::UndefInitializer, ::Int64, ::Int64)\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:146\n",
      " [8] ones(::Type, ::Int64, ::Vararg{Int64})\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:772\n",
      " [9] top-level scope\n",
      "   @ c:\\Users\\chev8\\.julia\\dev\\ParallelNotes\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sZmlsZQ==.jl:8"
     ]
    }
   ],
   "source": [
    "n1 = 1000\n",
    "v1 = CUDA.ones(Int, n1,n1)\n",
    "\n",
    "n2 = 10000\n",
    "v2 = CUDA.ones(Int, n2,n2)\n",
    "\n",
    "n3 = 100000\n",
    "v3 = CUDA.ones(Int, n3,n3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1: CuArrays for matrix-matrix products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a series of matrix-matirx + broadcasted scalar addition computaions. These examples were chosen to highlight GPU efficiency. We run the computations on the CPU and GPU and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat_mat_product_cpu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function mat_mat_product_cpu(A::Matrix{Float32},x::Matrix{Float32},y::Matrix{Float32})\n",
    "    b  = Float32(1)\n",
    "    y .= A*x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat_mat_product_gpu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mat_mat_product_gpu(A::CuArray{Float32, 2},x::CuArray{Float32, 2},y::CuArray{Float32, 2})\n",
    "    b  = Float32(1)\n",
    "    y .= A*x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.004621715732826844, 0.007289829512051735, 0.013411758400771745, 0.3292682926829268, 5.0810126582278485, 11.99789029535865, 18.81358859939542, 19.247947224366623, 18.109388202159288]\n"
     ]
    }
   ],
   "source": [
    "mat_size = [3; 5; 10; 100; 500; 1000; 5000; 10000; 15000]\n",
    "tcpu = zeros(length(mat_size))\n",
    "tgpu = zeros(length(mat_size))\n",
    "A = []\n",
    "x = []\n",
    "y = []\n",
    "ii = 1\n",
    "\n",
    "for mm in mat_size\n",
    "    A = Float32.(randn(mm,mm))\n",
    "    x = Float32.(randn(mm,50))\n",
    "    y = Float32.(randn(mm,50))\n",
    "    tcpu[ii] = @belapsed mat_mat_product_cpu(A,x,y)\n",
    "\n",
    "    A = CuArray(Float32.(randn(mm,mm)))\n",
    "    x = CuArray(Float32.(randn(mm,50)))\n",
    "    y = CuArray(Float32.(randn(mm,50)))\n",
    "    tgpu[ii] = @belapsed CUDA.@sync mat_mat_product_gpu(A,x,y)\n",
    "    ii += 1\n",
    "    println()\n",
    "end\n",
    "\n",
    "println(tcpu./tgpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the result, in log-log scale, to show GPU speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip490\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip491\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M251.071 1410.9 L2352.76 1410.9 L2352.76 47.2441 L251.071 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip492\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2103\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1126.85,1410.9 1126.85,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2198.89,1410.9 2198.89,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,1253.17 2352.76,1253.17 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,542.332 2352.76,542.332 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1126.85,1410.9 1126.85,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2198.89,1410.9 2198.89,1392 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M1092.62 1485.02 L1100.26 1485.02 L1100.26 1458.66 L1091.95 1460.32 L1091.95 1456.06 L1100.21 1454.4 L1104.89 1454.4 L1104.89 1485.02 L1112.53 1485.02 L1112.53 1488.96 L1092.62 1488.96 L1092.62 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1131.97 1457.48 Q1128.36 1457.48 1126.53 1461.04 Q1124.73 1464.58 1124.73 1471.71 Q1124.73 1478.82 1126.53 1482.38 Q1128.36 1485.92 1131.97 1485.92 Q1135.6 1485.92 1137.41 1482.38 Q1139.24 1478.82 1139.24 1471.71 Q1139.24 1464.58 1137.41 1461.04 Q1135.6 1457.48 1131.97 1457.48 M1131.97 1453.77 Q1137.78 1453.77 1140.84 1458.38 Q1143.91 1462.96 1143.91 1471.71 Q1143.91 1480.44 1140.84 1485.04 Q1137.78 1489.63 1131.97 1489.63 Q1126.16 1489.63 1123.08 1485.04 Q1120.03 1480.44 1120.03 1471.71 Q1120.03 1462.96 1123.08 1458.38 Q1126.16 1453.77 1131.97 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1148.49 1458.35 L1161.74 1458.35 L1161.74 1461.55 L1143.91 1461.55 L1143.91 1458.35 Q1146.08 1456.11 1149.8 1452.35 Q1153.54 1448.57 1154.5 1447.48 Q1156.33 1445.43 1157.04 1444.02 Q1157.78 1442.59 1157.78 1441.22 Q1157.78 1438.98 1156.2 1437.57 Q1154.64 1436.16 1152.12 1436.16 Q1150.33 1436.16 1148.33 1436.78 Q1146.36 1437.4 1144.1 1438.66 L1144.1 1434.82 Q1146.4 1433.9 1148.39 1433.43 Q1150.38 1432.96 1152.04 1432.96 Q1156.4 1432.96 1159 1435.14 Q1161.59 1437.32 1161.59 1440.97 Q1161.59 1442.7 1160.94 1444.26 Q1160.3 1445.8 1158.58 1447.91 Q1158.11 1448.46 1155.59 1451.07 Q1153.07 1453.67 1148.49 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2163.34 1485.02 L2170.98 1485.02 L2170.98 1458.66 L2162.67 1460.32 L2162.67 1456.06 L2170.93 1454.4 L2175.61 1454.4 L2175.61 1485.02 L2183.25 1485.02 L2183.25 1488.96 L2163.34 1488.96 L2163.34 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2202.69 1457.48 Q2199.08 1457.48 2197.25 1461.04 Q2195.45 1464.58 2195.45 1471.71 Q2195.45 1478.82 2197.25 1482.38 Q2199.08 1485.92 2202.69 1485.92 Q2206.33 1485.92 2208.13 1482.38 Q2209.96 1478.82 2209.96 1471.71 Q2209.96 1464.58 2208.13 1461.04 Q2206.33 1457.48 2202.69 1457.48 M2202.69 1453.77 Q2208.5 1453.77 2211.56 1458.38 Q2214.64 1462.96 2214.64 1471.71 Q2214.64 1480.44 2211.56 1485.04 Q2208.5 1489.63 2202.69 1489.63 Q2196.88 1489.63 2193.8 1485.04 Q2190.75 1480.44 2190.75 1471.71 Q2190.75 1462.96 2193.8 1458.38 Q2196.88 1453.77 2202.69 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2227.31 1436.78 L2217.72 1451.77 L2227.31 1451.77 L2227.31 1436.78 M2226.32 1433.47 L2231.09 1433.47 L2231.09 1451.77 L2235.1 1451.77 L2235.1 1454.93 L2231.09 1454.93 L2231.09 1461.55 L2227.31 1461.55 L2227.31 1454.93 L2214.64 1454.93 L2214.64 1451.26 L2226.32 1433.47 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1154.79 1545.38 Q1156.98 1541.43 1160.04 1539.56 Q1163.09 1537.68 1167.23 1537.68 Q1172.8 1537.68 1175.82 1541.59 Q1178.85 1545.48 1178.85 1552.67 L1178.85 1574.19 L1172.96 1574.19 L1172.96 1552.86 Q1172.96 1547.74 1171.15 1545.25 Q1169.33 1542.77 1165.61 1542.77 Q1161.06 1542.77 1158.41 1545.79 Q1155.77 1548.82 1155.77 1554.04 L1155.77 1574.19 L1149.88 1574.19 L1149.88 1552.86 Q1149.88 1547.7 1148.07 1545.25 Q1146.26 1542.77 1142.47 1542.77 Q1137.98 1542.77 1135.34 1545.83 Q1132.7 1548.85 1132.7 1554.04 L1132.7 1574.19 L1126.81 1574.19 L1126.81 1538.54 L1132.7 1538.54 L1132.7 1544.08 Q1134.7 1540.8 1137.5 1539.24 Q1140.3 1537.68 1144.16 1537.68 Q1148.04 1537.68 1150.74 1539.65 Q1153.48 1541.62 1154.79 1545.38 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1206.73 1556.27 Q1199.63 1556.27 1196.9 1557.89 Q1194.16 1559.51 1194.16 1563.43 Q1194.16 1566.55 1196.2 1568.39 Q1198.26 1570.21 1201.8 1570.21 Q1206.67 1570.21 1209.59 1566.77 Q1212.55 1563.3 1212.55 1557.57 L1212.55 1556.27 L1206.73 1556.27 M1218.41 1553.85 L1218.41 1574.19 L1212.55 1574.19 L1212.55 1568.77 Q1210.55 1572.02 1207.56 1573.58 Q1204.57 1575.11 1200.24 1575.11 Q1194.76 1575.11 1191.52 1572.05 Q1188.3 1568.97 1188.3 1563.81 Q1188.3 1557.79 1192.31 1554.74 Q1196.35 1551.68 1204.34 1551.68 L1212.55 1551.68 L1212.55 1551.11 Q1212.55 1547.07 1209.88 1544.87 Q1207.24 1542.64 1202.43 1542.64 Q1199.38 1542.64 1196.48 1543.38 Q1193.59 1544.11 1190.91 1545.57 L1190.91 1540.16 Q1194.13 1538.92 1197.15 1538.31 Q1200.17 1537.68 1203.04 1537.68 Q1210.77 1537.68 1214.59 1541.69 Q1218.41 1545.7 1218.41 1553.85 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1236.27 1528.42 L1236.27 1538.54 L1248.33 1538.54 L1248.33 1543.09 L1236.27 1543.09 L1236.27 1562.44 Q1236.27 1566.8 1237.44 1568.04 Q1238.65 1569.28 1242.31 1569.28 L1248.33 1569.28 L1248.33 1574.19 L1242.31 1574.19 Q1235.54 1574.19 1232.96 1571.67 Q1230.38 1569.12 1230.38 1562.44 L1230.38 1543.09 L1226.08 1543.09 L1226.08 1538.54 L1230.38 1538.54 L1230.38 1528.42 L1236.27 1528.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1276.69 1544.01 Q1275.7 1543.44 1274.53 1543.18 Q1273.38 1542.9 1271.98 1542.9 Q1267.01 1542.9 1264.34 1546.14 Q1261.7 1549.36 1261.7 1555.41 L1261.7 1574.19 L1255.81 1574.19 L1255.81 1538.54 L1261.7 1538.54 L1261.7 1544.08 Q1263.54 1540.83 1266.5 1539.27 Q1269.46 1537.68 1273.7 1537.68 Q1274.3 1537.68 1275.03 1537.77 Q1275.77 1537.84 1276.66 1538 L1276.69 1544.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1282.83 1538.54 L1288.69 1538.54 L1288.69 1574.19 L1282.83 1574.19 L1282.83 1538.54 M1282.83 1524.66 L1288.69 1524.66 L1288.69 1532.08 L1282.83 1532.08 L1282.83 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1330.58 1538.54 L1317.68 1555.88 L1331.24 1574.19 L1324.34 1574.19 L1313.96 1560.18 L1303.58 1574.19 L1296.68 1574.19 L1310.52 1555.53 L1297.86 1538.54 L1304.76 1538.54 L1314.22 1551.24 L1323.67 1538.54 L1330.58 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1382.96 1539.59 L1382.96 1545.13 Q1380.48 1543.85 1377.81 1543.22 Q1375.14 1542.58 1372.27 1542.58 Q1367.91 1542.58 1365.71 1543.92 Q1363.55 1545.25 1363.55 1547.93 Q1363.55 1549.96 1365.11 1551.14 Q1366.67 1552.29 1371.38 1553.34 L1373.38 1553.78 Q1379.62 1555.12 1382.23 1557.57 Q1384.87 1559.99 1384.87 1564.35 Q1384.87 1569.32 1380.93 1572.21 Q1377.01 1575.11 1370.14 1575.11 Q1367.27 1575.11 1364.15 1574.54 Q1361.07 1573.99 1357.63 1572.88 L1357.63 1566.83 Q1360.88 1568.52 1364.03 1569.38 Q1367.18 1570.21 1370.27 1570.21 Q1374.4 1570.21 1376.63 1568.81 Q1378.86 1567.37 1378.86 1564.8 Q1378.86 1562.41 1377.24 1561.14 Q1375.64 1559.86 1370.2 1558.68 L1368.16 1558.21 Q1362.72 1557.06 1360.3 1554.71 Q1357.88 1552.32 1357.88 1548.18 Q1357.88 1543.15 1361.45 1540.42 Q1365.01 1537.68 1371.57 1537.68 Q1374.82 1537.68 1377.68 1538.16 Q1380.55 1538.63 1382.96 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1394.2 1538.54 L1400.06 1538.54 L1400.06 1574.19 L1394.2 1574.19 L1394.2 1538.54 M1394.2 1524.66 L1400.06 1524.66 L1400.06 1532.08 L1394.2 1532.08 L1394.2 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1409.76 1538.54 L1437.58 1538.54 L1437.58 1543.88 L1415.56 1569.51 L1437.58 1569.51 L1437.58 1574.19 L1408.97 1574.19 L1408.97 1568.84 L1430.99 1543.22 L1409.76 1543.22 L1409.76 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1477.02 1554.9 L1477.02 1557.76 L1450.09 1557.76 Q1450.47 1563.81 1453.72 1566.99 Q1457 1570.14 1462.82 1570.14 Q1466.2 1570.14 1469.35 1569.32 Q1472.53 1568.49 1475.65 1566.83 L1475.65 1572.37 Q1472.5 1573.71 1469.19 1574.41 Q1465.88 1575.11 1462.47 1575.11 Q1453.94 1575.11 1448.95 1570.14 Q1443.98 1565.18 1443.98 1556.71 Q1443.98 1547.96 1448.69 1542.83 Q1453.43 1537.68 1461.45 1537.68 Q1468.65 1537.68 1472.82 1542.33 Q1477.02 1546.94 1477.02 1554.9 M1471.16 1553.18 Q1471.1 1548.37 1468.46 1545.51 Q1465.85 1542.64 1461.52 1542.64 Q1456.62 1542.64 1453.66 1545.41 Q1450.73 1548.18 1450.28 1553.21 L1471.16 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 251.071,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1253.17 269.969,1253.17 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,542.332 269.969,542.332 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M114.931 1272.96 L122.57 1272.96 L122.57 1246.59 L114.26 1248.26 L114.26 1244 L122.524 1242.34 L127.2 1242.34 L127.2 1272.96 L134.839 1272.96 L134.839 1276.9 L114.931 1276.9 L114.931 1272.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M154.283 1245.41 Q150.672 1245.41 148.843 1248.98 Q147.038 1252.52 147.038 1259.65 Q147.038 1266.76 148.843 1270.32 Q150.672 1273.86 154.283 1273.86 Q157.917 1273.86 159.723 1270.32 Q161.552 1266.76 161.552 1259.65 Q161.552 1252.52 159.723 1248.98 Q157.917 1245.41 154.283 1245.41 M154.283 1241.71 Q160.093 1241.71 163.149 1246.32 Q166.227 1250.9 166.227 1259.65 Q166.227 1268.38 163.149 1272.98 Q160.093 1277.57 154.283 1277.57 Q148.473 1277.57 145.394 1272.98 Q142.339 1268.38 142.339 1259.65 Q142.339 1250.9 145.394 1246.32 Q148.473 1241.71 154.283 1241.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M166.227 1235.81 L190.339 1235.81 L190.339 1239.01 L166.227 1239.01 L166.227 1235.81 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M201.812 1246.29 L215.071 1246.29 L215.071 1249.48 L197.241 1249.48 L197.241 1246.29 Q199.404 1244.05 203.128 1240.29 Q206.871 1236.51 207.83 1235.42 Q209.655 1233.37 210.369 1231.96 Q211.103 1230.53 211.103 1229.15 Q211.103 1226.92 209.523 1225.5 Q207.962 1224.09 205.442 1224.09 Q203.655 1224.09 201.661 1224.72 Q199.686 1225.34 197.43 1226.6 L197.43 1222.76 Q199.724 1221.84 201.718 1221.37 Q203.711 1220.9 205.366 1220.9 Q209.73 1220.9 212.325 1223.08 Q214.921 1225.26 214.921 1228.91 Q214.921 1230.64 214.263 1232.2 Q213.623 1233.74 211.912 1235.85 Q211.441 1236.39 208.921 1239.01 Q206.401 1241.6 201.812 1246.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M144.366 562.124 L152.004 562.124 L152.004 535.758 L143.694 537.425 L143.694 533.166 L151.958 531.499 L156.634 531.499 L156.634 562.124 L164.273 562.124 L164.273 566.059 L144.366 566.059 L144.366 562.124 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M183.717 534.578 Q180.106 534.578 178.277 538.143 Q176.472 541.684 176.472 548.814 Q176.472 555.92 178.277 559.485 Q180.106 563.027 183.717 563.027 Q187.351 563.027 189.157 559.485 Q190.986 555.92 190.986 548.814 Q190.986 541.684 189.157 538.143 Q187.351 534.578 183.717 534.578 M183.717 530.874 Q189.527 530.874 192.583 535.481 Q195.662 540.064 195.662 548.814 Q195.662 557.541 192.583 562.147 Q189.527 566.73 183.717 566.73 Q177.907 566.73 174.828 562.147 Q171.773 557.541 171.773 548.814 Q171.773 540.064 174.828 535.481 Q177.907 530.874 183.717 530.874 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M205.366 513.07 Q202.432 513.07 200.947 515.967 Q199.48 518.844 199.48 524.637 Q199.48 530.411 200.947 533.307 Q202.432 536.185 205.366 536.185 Q208.319 536.185 209.786 533.307 Q211.272 530.411 211.272 524.637 Q211.272 518.844 209.786 515.967 Q208.319 513.07 205.366 513.07 M205.366 510.061 Q210.087 510.061 212.57 513.804 Q215.071 517.528 215.071 524.637 Q215.071 531.727 212.57 535.47 Q210.087 539.194 205.366 539.194 Q200.646 539.194 198.144 535.47 Q195.662 531.727 195.662 524.637 Q195.662 517.528 198.144 513.804 Q200.646 510.061 205.366 510.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M57.2247 909.252 L44.4614 909.252 L44.4614 919.755 L39.1779 919.755 L39.1779 902.886 L59.58 902.886 Q62.2218 906.61 63.5904 911.098 Q64.9272 915.586 64.9272 920.678 Q64.9272 931.818 58.4342 938.12 Q51.9093 944.39 40.2919 944.39 Q28.6427 944.39 22.1496 938.12 Q15.6248 931.818 15.6248 920.678 Q15.6248 916.031 16.7706 911.862 Q17.9164 907.66 20.1444 904.127 L26.9876 904.127 Q23.9639 907.692 22.4361 911.702 Q20.9083 915.713 20.9083 920.137 Q20.9083 928.858 25.7781 933.25 Q30.6479 937.611 40.2919 937.611 Q49.9041 937.611 54.7739 933.25 Q59.6436 928.858 59.6436 920.137 Q59.6436 916.731 59.0707 914.058 Q58.466 911.384 57.2247 909.252 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M21.7677 884.712 L39.6235 884.712 L39.6235 876.627 Q39.6235 872.14 37.3 869.689 Q34.9765 867.238 30.6797 867.238 Q26.4147 867.238 24.0912 869.689 Q21.7677 872.14 21.7677 876.627 L21.7677 884.712 M16.4842 891.141 L16.4842 876.627 Q16.4842 868.638 20.1126 864.564 Q23.7092 860.459 30.6797 860.459 Q37.7138 860.459 41.3104 864.564 Q44.907 868.638 44.907 876.627 L44.907 884.712 L64.0042 884.712 L64.0042 891.141 L16.4842 891.141 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M16.4842 852.565 L16.4842 846.104 L45.3526 846.104 Q52.9915 846.104 56.3653 843.335 Q59.7073 840.566 59.7073 834.359 Q59.7073 828.184 56.3653 825.415 Q52.9915 822.646 45.3526 822.646 L16.4842 822.646 L16.4842 816.185 L46.1484 816.185 Q55.4423 816.185 60.1847 820.8 Q64.9272 825.384 64.9272 834.359 Q64.9272 843.367 60.1847 847.982 Q55.4423 852.565 46.1484 852.565 L16.4842 852.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M29.4065 760.931 L34.9447 760.931 Q33.6716 763.413 33.035 766.087 Q32.3984 768.761 32.3984 771.625 Q32.3984 775.986 33.7352 778.182 Q35.072 780.346 37.7456 780.346 Q39.7826 780.346 40.9603 778.787 Q42.1061 777.227 43.1565 772.516 L43.6021 770.511 Q44.9389 764.273 47.3897 761.663 Q49.8086 759.021 54.1691 759.021 Q59.1344 759.021 62.0308 762.968 Q64.9272 766.883 64.9272 773.758 Q64.9272 776.622 64.3543 779.741 Q63.8132 782.829 62.6992 786.266 L56.6518 786.266 Q58.3387 783.02 59.198 779.869 Q60.0256 776.718 60.0256 773.63 Q60.0256 769.493 58.6251 767.265 Q57.1929 765.037 54.6147 765.037 Q52.2276 765.037 50.9545 766.66 Q49.6813 768.251 48.5037 773.694 L48.0262 775.731 Q46.8804 781.174 44.5251 783.593 Q42.138 786.012 38.0002 786.012 Q32.9713 786.012 30.2341 782.447 Q27.4968 778.882 27.4968 772.325 Q27.4968 769.079 27.9743 766.214 Q28.4517 763.35 29.4065 760.931 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M58.657 744.03 L77.5631 744.03 L77.5631 749.918 L28.3562 749.918 L28.3562 744.03 L33.7671 744.03 Q30.5842 742.184 29.0564 739.383 Q27.4968 736.55 27.4968 732.635 Q27.4968 726.142 32.6531 722.1 Q37.8093 718.026 46.212 718.026 Q54.6147 718.026 59.771 722.1 Q64.9272 726.142 64.9272 732.635 Q64.9272 736.55 63.3994 739.383 Q61.8398 742.184 58.657 744.03 M46.212 724.105 Q39.7508 724.105 36.0905 726.779 Q32.3984 729.42 32.3984 734.067 Q32.3984 738.714 36.0905 741.388 Q39.7508 744.03 46.212 744.03 Q52.6732 744.03 56.3653 741.388 Q60.0256 738.714 60.0256 734.067 Q60.0256 729.42 56.3653 726.779 Q52.6732 724.105 46.212 724.105 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M44.7161 677.826 L47.5806 677.826 L47.5806 704.753 Q53.6281 704.371 56.8109 701.125 Q59.9619 697.847 59.9619 692.022 Q59.9619 688.648 59.1344 685.497 Q58.3069 682.314 56.6518 679.195 L62.1899 679.195 Q63.5267 682.346 64.227 685.656 Q64.9272 688.966 64.9272 692.372 Q64.9272 700.902 59.9619 705.899 Q54.9967 710.864 46.5303 710.864 Q37.7774 710.864 32.6531 706.154 Q27.4968 701.411 27.4968 693.391 Q27.4968 686.197 32.1438 682.028 Q36.7589 677.826 44.7161 677.826 M42.9973 683.683 Q38.1912 683.747 35.3266 686.388 Q32.4621 688.998 32.4621 693.327 Q32.4621 698.229 35.2312 701.189 Q38.0002 704.117 43.0292 704.562 L42.9973 683.683 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M44.7161 637.722 L47.5806 637.722 L47.5806 664.649 Q53.6281 664.267 56.8109 661.021 Q59.9619 657.743 59.9619 651.918 Q59.9619 648.544 59.1344 645.393 Q58.3069 642.21 56.6518 639.091 L62.1899 639.091 Q63.5267 642.242 64.227 645.552 Q64.9272 648.862 64.9272 652.268 Q64.9272 660.798 59.9619 665.795 Q54.9967 670.76 46.5303 670.76 Q37.7774 670.76 32.6531 666.05 Q27.4968 661.307 27.4968 653.287 Q27.4968 646.093 32.1438 641.924 Q36.7589 637.722 44.7161 637.722 M42.9973 643.579 Q38.1912 643.643 35.3266 646.284 Q32.4621 648.894 32.4621 653.223 Q32.4621 658.125 35.2312 661.085 Q38.0002 664.013 43.0292 664.458 L42.9973 643.579 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M33.7671 604.653 L14.479 604.653 L14.479 598.796 L64.0042 598.796 L64.0042 604.653 L58.657 604.653 Q61.8398 606.499 63.3994 609.331 Q64.9272 612.132 64.9272 616.079 Q64.9272 622.54 59.771 626.614 Q54.6147 630.657 46.212 630.657 Q37.8093 630.657 32.6531 626.614 Q27.4968 622.54 27.4968 616.079 Q27.4968 612.132 29.0564 609.331 Q30.5842 606.499 33.7671 604.653 M46.212 624.609 Q52.6732 624.609 56.3653 621.967 Q60.0256 619.294 60.0256 614.647 Q60.0256 610 56.3653 607.326 Q52.6732 604.653 46.212 604.653 Q39.7508 604.653 36.0905 607.326 Q32.3984 610 32.3984 614.647 Q32.3984 619.294 36.0905 621.967 Q39.7508 624.609 46.212 624.609 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M49.9359 587.338 L28.3562 587.338 L28.3562 581.481 L49.7131 581.481 Q54.7739 581.481 57.3202 579.508 Q59.8346 577.535 59.8346 573.588 Q59.8346 568.846 56.8109 566.108 Q53.7872 563.339 48.5673 563.339 L28.3562 563.339 L28.3562 557.483 L64.0042 557.483 L64.0042 563.339 L58.5296 563.339 Q61.7762 565.472 63.3676 568.304 Q64.9272 571.105 64.9272 574.829 Q64.9272 580.972 61.1078 584.155 Q57.2883 587.338 49.9359 587.338 M27.4968 572.601 L27.4968 572.601 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M58.657 539.754 L77.5631 539.754 L77.5631 545.643 L28.3562 545.643 L28.3562 539.754 L33.7671 539.754 Q30.5842 537.908 29.0564 535.107 Q27.4968 532.275 27.4968 528.36 Q27.4968 521.867 32.6531 517.824 Q37.8093 513.75 46.212 513.75 Q54.6147 513.75 59.771 517.824 Q64.9272 521.867 64.9272 528.36 Q64.9272 532.275 63.3994 535.107 Q61.8398 537.908 58.657 539.754 M46.212 519.83 Q39.7508 519.83 36.0905 522.503 Q32.3984 525.145 32.3984 529.792 Q32.3984 534.439 36.0905 537.112 Q39.7508 539.754 46.212 539.754 Q52.6732 539.754 56.3653 537.112 Q60.0256 534.439 60.0256 529.792 Q60.0256 525.145 56.3653 522.503 Q52.6732 519.83 46.212 519.83 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip492)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"310.553,1372.3 429.468,1301.96 590.826,1207.86 1126.85,713.803 1501.51,291.424 1662.87,158.798 2037.53,89.3612 2198.89,85.838 2293.27,95.2497 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M2020.1 196.379 L2282.7 196.379 L2282.7 92.6992 L2020.1 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2020.1,196.379 2282.7,196.379 2282.7,92.6992 2020.1,92.6992 2020.1,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2043.45,144.539 2183.57,144.539 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M2220.76 164.227 Q2218.95 168.856 2217.24 170.268 Q2215.53 171.68 2212.66 171.68 L2209.26 171.68 L2209.26 168.115 L2211.76 168.115 Q2213.51 168.115 2214.49 167.282 Q2215.46 166.449 2216.64 163.347 L2217.4 161.403 L2206.92 135.893 L2211.43 135.893 L2219.53 156.171 L2227.63 135.893 L2232.15 135.893 L2220.76 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2239.44 157.884 L2247.08 157.884 L2247.08 131.518 L2238.77 133.185 L2238.77 128.926 L2247.03 127.259 L2251.71 127.259 L2251.71 157.884 L2259.35 157.884 L2259.35 161.819 L2239.44 161.819 L2239.44 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip540\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip541\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M251.071 1410.9 L2352.76 1410.9 L2352.76 47.2441 L251.071 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip542\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2103\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip542)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1126.85,1410.9 1126.85,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip542)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2198.89,1410.9 2198.89,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip542)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,1253.17 2352.76,1253.17 \"/>\n",
       "<polyline clip-path=\"url(#clip542)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,542.332 2352.76,542.332 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1126.85,1410.9 1126.85,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2198.89,1410.9 2198.89,1392 \"/>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M1092.62 1485.02 L1100.26 1485.02 L1100.26 1458.66 L1091.95 1460.32 L1091.95 1456.06 L1100.21 1454.4 L1104.89 1454.4 L1104.89 1485.02 L1112.53 1485.02 L1112.53 1488.96 L1092.62 1488.96 L1092.62 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1131.97 1457.48 Q1128.36 1457.48 1126.53 1461.04 Q1124.73 1464.58 1124.73 1471.71 Q1124.73 1478.82 1126.53 1482.38 Q1128.36 1485.92 1131.97 1485.92 Q1135.6 1485.92 1137.41 1482.38 Q1139.24 1478.82 1139.24 1471.71 Q1139.24 1464.58 1137.41 1461.04 Q1135.6 1457.48 1131.97 1457.48 M1131.97 1453.77 Q1137.78 1453.77 1140.84 1458.38 Q1143.91 1462.96 1143.91 1471.71 Q1143.91 1480.44 1140.84 1485.04 Q1137.78 1489.63 1131.97 1489.63 Q1126.16 1489.63 1123.08 1485.04 Q1120.03 1480.44 1120.03 1471.71 Q1120.03 1462.96 1123.08 1458.38 Q1126.16 1453.77 1131.97 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1148.49 1458.35 L1161.74 1458.35 L1161.74 1461.55 L1143.91 1461.55 L1143.91 1458.35 Q1146.08 1456.11 1149.8 1452.35 Q1153.54 1448.57 1154.5 1447.48 Q1156.33 1445.43 1157.04 1444.02 Q1157.78 1442.59 1157.78 1441.22 Q1157.78 1438.98 1156.2 1437.57 Q1154.64 1436.16 1152.12 1436.16 Q1150.33 1436.16 1148.33 1436.78 Q1146.36 1437.4 1144.1 1438.66 L1144.1 1434.82 Q1146.4 1433.9 1148.39 1433.43 Q1150.38 1432.96 1152.04 1432.96 Q1156.4 1432.96 1159 1435.14 Q1161.59 1437.32 1161.59 1440.97 Q1161.59 1442.7 1160.94 1444.26 Q1160.3 1445.8 1158.58 1447.91 Q1158.11 1448.46 1155.59 1451.07 Q1153.07 1453.67 1148.49 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M2163.34 1485.02 L2170.98 1485.02 L2170.98 1458.66 L2162.67 1460.32 L2162.67 1456.06 L2170.93 1454.4 L2175.61 1454.4 L2175.61 1485.02 L2183.25 1485.02 L2183.25 1488.96 L2163.34 1488.96 L2163.34 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M2202.69 1457.48 Q2199.08 1457.48 2197.25 1461.04 Q2195.45 1464.58 2195.45 1471.71 Q2195.45 1478.82 2197.25 1482.38 Q2199.08 1485.92 2202.69 1485.92 Q2206.33 1485.92 2208.13 1482.38 Q2209.96 1478.82 2209.96 1471.71 Q2209.96 1464.58 2208.13 1461.04 Q2206.33 1457.48 2202.69 1457.48 M2202.69 1453.77 Q2208.5 1453.77 2211.56 1458.38 Q2214.64 1462.96 2214.64 1471.71 Q2214.64 1480.44 2211.56 1485.04 Q2208.5 1489.63 2202.69 1489.63 Q2196.88 1489.63 2193.8 1485.04 Q2190.75 1480.44 2190.75 1471.71 Q2190.75 1462.96 2193.8 1458.38 Q2196.88 1453.77 2202.69 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M2227.31 1436.78 L2217.72 1451.77 L2227.31 1451.77 L2227.31 1436.78 M2226.32 1433.47 L2231.09 1433.47 L2231.09 1451.77 L2235.1 1451.77 L2235.1 1454.93 L2231.09 1454.93 L2231.09 1461.55 L2227.31 1461.55 L2227.31 1454.93 L2214.64 1454.93 L2214.64 1451.26 L2226.32 1433.47 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1154.79 1545.38 Q1156.98 1541.43 1160.04 1539.56 Q1163.09 1537.68 1167.23 1537.68 Q1172.8 1537.68 1175.82 1541.59 Q1178.85 1545.48 1178.85 1552.67 L1178.85 1574.19 L1172.96 1574.19 L1172.96 1552.86 Q1172.96 1547.74 1171.15 1545.25 Q1169.33 1542.77 1165.61 1542.77 Q1161.06 1542.77 1158.41 1545.79 Q1155.77 1548.82 1155.77 1554.04 L1155.77 1574.19 L1149.88 1574.19 L1149.88 1552.86 Q1149.88 1547.7 1148.07 1545.25 Q1146.26 1542.77 1142.47 1542.77 Q1137.98 1542.77 1135.34 1545.83 Q1132.7 1548.85 1132.7 1554.04 L1132.7 1574.19 L1126.81 1574.19 L1126.81 1538.54 L1132.7 1538.54 L1132.7 1544.08 Q1134.7 1540.8 1137.5 1539.24 Q1140.3 1537.68 1144.16 1537.68 Q1148.04 1537.68 1150.74 1539.65 Q1153.48 1541.62 1154.79 1545.38 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1206.73 1556.27 Q1199.63 1556.27 1196.9 1557.89 Q1194.16 1559.51 1194.16 1563.43 Q1194.16 1566.55 1196.2 1568.39 Q1198.26 1570.21 1201.8 1570.21 Q1206.67 1570.21 1209.59 1566.77 Q1212.55 1563.3 1212.55 1557.57 L1212.55 1556.27 L1206.73 1556.27 M1218.41 1553.85 L1218.41 1574.19 L1212.55 1574.19 L1212.55 1568.77 Q1210.55 1572.02 1207.56 1573.58 Q1204.57 1575.11 1200.24 1575.11 Q1194.76 1575.11 1191.52 1572.05 Q1188.3 1568.97 1188.3 1563.81 Q1188.3 1557.79 1192.31 1554.74 Q1196.35 1551.68 1204.34 1551.68 L1212.55 1551.68 L1212.55 1551.11 Q1212.55 1547.07 1209.88 1544.87 Q1207.24 1542.64 1202.43 1542.64 Q1199.38 1542.64 1196.48 1543.38 Q1193.59 1544.11 1190.91 1545.57 L1190.91 1540.16 Q1194.13 1538.92 1197.15 1538.31 Q1200.17 1537.68 1203.04 1537.68 Q1210.77 1537.68 1214.59 1541.69 Q1218.41 1545.7 1218.41 1553.85 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1236.27 1528.42 L1236.27 1538.54 L1248.33 1538.54 L1248.33 1543.09 L1236.27 1543.09 L1236.27 1562.44 Q1236.27 1566.8 1237.44 1568.04 Q1238.65 1569.28 1242.31 1569.28 L1248.33 1569.28 L1248.33 1574.19 L1242.31 1574.19 Q1235.54 1574.19 1232.96 1571.67 Q1230.38 1569.12 1230.38 1562.44 L1230.38 1543.09 L1226.08 1543.09 L1226.08 1538.54 L1230.38 1538.54 L1230.38 1528.42 L1236.27 1528.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1276.69 1544.01 Q1275.7 1543.44 1274.53 1543.18 Q1273.38 1542.9 1271.98 1542.9 Q1267.01 1542.9 1264.34 1546.14 Q1261.7 1549.36 1261.7 1555.41 L1261.7 1574.19 L1255.81 1574.19 L1255.81 1538.54 L1261.7 1538.54 L1261.7 1544.08 Q1263.54 1540.83 1266.5 1539.27 Q1269.46 1537.68 1273.7 1537.68 Q1274.3 1537.68 1275.03 1537.77 Q1275.77 1537.84 1276.66 1538 L1276.69 1544.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1282.83 1538.54 L1288.69 1538.54 L1288.69 1574.19 L1282.83 1574.19 L1282.83 1538.54 M1282.83 1524.66 L1288.69 1524.66 L1288.69 1532.08 L1282.83 1532.08 L1282.83 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1330.58 1538.54 L1317.68 1555.88 L1331.24 1574.19 L1324.34 1574.19 L1313.96 1560.18 L1303.58 1574.19 L1296.68 1574.19 L1310.52 1555.53 L1297.86 1538.54 L1304.76 1538.54 L1314.22 1551.24 L1323.67 1538.54 L1330.58 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1382.96 1539.59 L1382.96 1545.13 Q1380.48 1543.85 1377.81 1543.22 Q1375.14 1542.58 1372.27 1542.58 Q1367.91 1542.58 1365.71 1543.92 Q1363.55 1545.25 1363.55 1547.93 Q1363.55 1549.96 1365.11 1551.14 Q1366.67 1552.29 1371.38 1553.34 L1373.38 1553.78 Q1379.62 1555.12 1382.23 1557.57 Q1384.87 1559.99 1384.87 1564.35 Q1384.87 1569.32 1380.93 1572.21 Q1377.01 1575.11 1370.14 1575.11 Q1367.27 1575.11 1364.15 1574.54 Q1361.07 1573.99 1357.63 1572.88 L1357.63 1566.83 Q1360.88 1568.52 1364.03 1569.38 Q1367.18 1570.21 1370.27 1570.21 Q1374.4 1570.21 1376.63 1568.81 Q1378.86 1567.37 1378.86 1564.8 Q1378.86 1562.41 1377.24 1561.14 Q1375.64 1559.86 1370.2 1558.68 L1368.16 1558.21 Q1362.72 1557.06 1360.3 1554.71 Q1357.88 1552.32 1357.88 1548.18 Q1357.88 1543.15 1361.45 1540.42 Q1365.01 1537.68 1371.57 1537.68 Q1374.82 1537.68 1377.68 1538.16 Q1380.55 1538.63 1382.96 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1394.2 1538.54 L1400.06 1538.54 L1400.06 1574.19 L1394.2 1574.19 L1394.2 1538.54 M1394.2 1524.66 L1400.06 1524.66 L1400.06 1532.08 L1394.2 1532.08 L1394.2 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1409.76 1538.54 L1437.58 1538.54 L1437.58 1543.88 L1415.56 1569.51 L1437.58 1569.51 L1437.58 1574.19 L1408.97 1574.19 L1408.97 1568.84 L1430.99 1543.22 L1409.76 1543.22 L1409.76 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M1477.02 1554.9 L1477.02 1557.76 L1450.09 1557.76 Q1450.47 1563.81 1453.72 1566.99 Q1457 1570.14 1462.82 1570.14 Q1466.2 1570.14 1469.35 1569.32 Q1472.53 1568.49 1475.65 1566.83 L1475.65 1572.37 Q1472.5 1573.71 1469.19 1574.41 Q1465.88 1575.11 1462.47 1575.11 Q1453.94 1575.11 1448.95 1570.14 Q1443.98 1565.18 1443.98 1556.71 Q1443.98 1547.96 1448.69 1542.83 Q1453.43 1537.68 1461.45 1537.68 Q1468.65 1537.68 1472.82 1542.33 Q1477.02 1546.94 1477.02 1554.9 M1471.16 1553.18 Q1471.1 1548.37 1468.46 1545.51 Q1465.85 1542.64 1461.52 1542.64 Q1456.62 1542.64 1453.66 1545.41 Q1450.73 1548.18 1450.28 1553.21 L1471.16 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 251.071,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1253.17 269.969,1253.17 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,542.332 269.969,542.332 \"/>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M114.931 1272.96 L122.57 1272.96 L122.57 1246.59 L114.26 1248.26 L114.26 1244 L122.524 1242.34 L127.2 1242.34 L127.2 1272.96 L134.839 1272.96 L134.839 1276.9 L114.931 1276.9 L114.931 1272.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M154.283 1245.41 Q150.672 1245.41 148.843 1248.98 Q147.038 1252.52 147.038 1259.65 Q147.038 1266.76 148.843 1270.32 Q150.672 1273.86 154.283 1273.86 Q157.917 1273.86 159.723 1270.32 Q161.552 1266.76 161.552 1259.65 Q161.552 1252.52 159.723 1248.98 Q157.917 1245.41 154.283 1245.41 M154.283 1241.71 Q160.093 1241.71 163.149 1246.32 Q166.227 1250.9 166.227 1259.65 Q166.227 1268.38 163.149 1272.98 Q160.093 1277.57 154.283 1277.57 Q148.473 1277.57 145.394 1272.98 Q142.339 1268.38 142.339 1259.65 Q142.339 1250.9 145.394 1246.32 Q148.473 1241.71 154.283 1241.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M166.227 1235.81 L190.339 1235.81 L190.339 1239.01 L166.227 1239.01 L166.227 1235.81 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M201.812 1246.29 L215.071 1246.29 L215.071 1249.48 L197.241 1249.48 L197.241 1246.29 Q199.404 1244.05 203.128 1240.29 Q206.871 1236.51 207.83 1235.42 Q209.655 1233.37 210.369 1231.96 Q211.103 1230.53 211.103 1229.15 Q211.103 1226.92 209.523 1225.5 Q207.962 1224.09 205.442 1224.09 Q203.655 1224.09 201.661 1224.72 Q199.686 1225.34 197.43 1226.6 L197.43 1222.76 Q199.724 1221.84 201.718 1221.37 Q203.711 1220.9 205.366 1220.9 Q209.73 1220.9 212.325 1223.08 Q214.921 1225.26 214.921 1228.91 Q214.921 1230.64 214.263 1232.2 Q213.623 1233.74 211.912 1235.85 Q211.441 1236.39 208.921 1239.01 Q206.401 1241.6 201.812 1246.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M144.366 562.124 L152.004 562.124 L152.004 535.758 L143.694 537.425 L143.694 533.166 L151.958 531.499 L156.634 531.499 L156.634 562.124 L164.273 562.124 L164.273 566.059 L144.366 566.059 L144.366 562.124 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M183.717 534.578 Q180.106 534.578 178.277 538.143 Q176.472 541.684 176.472 548.814 Q176.472 555.92 178.277 559.485 Q180.106 563.027 183.717 563.027 Q187.351 563.027 189.157 559.485 Q190.986 555.92 190.986 548.814 Q190.986 541.684 189.157 538.143 Q187.351 534.578 183.717 534.578 M183.717 530.874 Q189.527 530.874 192.583 535.481 Q195.662 540.064 195.662 548.814 Q195.662 557.541 192.583 562.147 Q189.527 566.73 183.717 566.73 Q177.907 566.73 174.828 562.147 Q171.773 557.541 171.773 548.814 Q171.773 540.064 174.828 535.481 Q177.907 530.874 183.717 530.874 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M205.366 513.07 Q202.432 513.07 200.947 515.967 Q199.48 518.844 199.48 524.637 Q199.48 530.411 200.947 533.307 Q202.432 536.185 205.366 536.185 Q208.319 536.185 209.786 533.307 Q211.272 530.411 211.272 524.637 Q211.272 518.844 209.786 515.967 Q208.319 513.07 205.366 513.07 M205.366 510.061 Q210.087 510.061 212.57 513.804 Q215.071 517.528 215.071 524.637 Q215.071 531.727 212.57 535.47 Q210.087 539.194 205.366 539.194 Q200.646 539.194 198.144 535.47 Q195.662 531.727 195.662 524.637 Q195.662 517.528 198.144 513.804 Q200.646 510.061 205.366 510.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M57.2247 909.252 L44.4614 909.252 L44.4614 919.755 L39.1779 919.755 L39.1779 902.886 L59.58 902.886 Q62.2218 906.61 63.5904 911.098 Q64.9272 915.586 64.9272 920.678 Q64.9272 931.818 58.4342 938.12 Q51.9093 944.39 40.2919 944.39 Q28.6427 944.39 22.1496 938.12 Q15.6248 931.818 15.6248 920.678 Q15.6248 916.031 16.7706 911.862 Q17.9164 907.66 20.1444 904.127 L26.9876 904.127 Q23.9639 907.692 22.4361 911.702 Q20.9083 915.713 20.9083 920.137 Q20.9083 928.858 25.7781 933.25 Q30.6479 937.611 40.2919 937.611 Q49.9041 937.611 54.7739 933.25 Q59.6436 928.858 59.6436 920.137 Q59.6436 916.731 59.0707 914.058 Q58.466 911.384 57.2247 909.252 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M21.7677 884.712 L39.6235 884.712 L39.6235 876.627 Q39.6235 872.14 37.3 869.689 Q34.9765 867.238 30.6797 867.238 Q26.4147 867.238 24.0912 869.689 Q21.7677 872.14 21.7677 876.627 L21.7677 884.712 M16.4842 891.141 L16.4842 876.627 Q16.4842 868.638 20.1126 864.564 Q23.7092 860.459 30.6797 860.459 Q37.7138 860.459 41.3104 864.564 Q44.907 868.638 44.907 876.627 L44.907 884.712 L64.0042 884.712 L64.0042 891.141 L16.4842 891.141 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M16.4842 852.565 L16.4842 846.104 L45.3526 846.104 Q52.9915 846.104 56.3653 843.335 Q59.7073 840.566 59.7073 834.359 Q59.7073 828.184 56.3653 825.415 Q52.9915 822.646 45.3526 822.646 L16.4842 822.646 L16.4842 816.185 L46.1484 816.185 Q55.4423 816.185 60.1847 820.8 Q64.9272 825.384 64.9272 834.359 Q64.9272 843.367 60.1847 847.982 Q55.4423 852.565 46.1484 852.565 L16.4842 852.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M29.4065 760.931 L34.9447 760.931 Q33.6716 763.413 33.035 766.087 Q32.3984 768.761 32.3984 771.625 Q32.3984 775.986 33.7352 778.182 Q35.072 780.346 37.7456 780.346 Q39.7826 780.346 40.9603 778.787 Q42.1061 777.227 43.1565 772.516 L43.6021 770.511 Q44.9389 764.273 47.3897 761.663 Q49.8086 759.021 54.1691 759.021 Q59.1344 759.021 62.0308 762.968 Q64.9272 766.883 64.9272 773.758 Q64.9272 776.622 64.3543 779.741 Q63.8132 782.829 62.6992 786.266 L56.6518 786.266 Q58.3387 783.02 59.198 779.869 Q60.0256 776.718 60.0256 773.63 Q60.0256 769.493 58.6251 767.265 Q57.1929 765.037 54.6147 765.037 Q52.2276 765.037 50.9545 766.66 Q49.6813 768.251 48.5037 773.694 L48.0262 775.731 Q46.8804 781.174 44.5251 783.593 Q42.138 786.012 38.0002 786.012 Q32.9713 786.012 30.2341 782.447 Q27.4968 778.882 27.4968 772.325 Q27.4968 769.079 27.9743 766.214 Q28.4517 763.35 29.4065 760.931 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M58.657 744.03 L77.5631 744.03 L77.5631 749.918 L28.3562 749.918 L28.3562 744.03 L33.7671 744.03 Q30.5842 742.184 29.0564 739.383 Q27.4968 736.55 27.4968 732.635 Q27.4968 726.142 32.6531 722.1 Q37.8093 718.026 46.212 718.026 Q54.6147 718.026 59.771 722.1 Q64.9272 726.142 64.9272 732.635 Q64.9272 736.55 63.3994 739.383 Q61.8398 742.184 58.657 744.03 M46.212 724.105 Q39.7508 724.105 36.0905 726.779 Q32.3984 729.42 32.3984 734.067 Q32.3984 738.714 36.0905 741.388 Q39.7508 744.03 46.212 744.03 Q52.6732 744.03 56.3653 741.388 Q60.0256 738.714 60.0256 734.067 Q60.0256 729.42 56.3653 726.779 Q52.6732 724.105 46.212 724.105 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M44.7161 677.826 L47.5806 677.826 L47.5806 704.753 Q53.6281 704.371 56.8109 701.125 Q59.9619 697.847 59.9619 692.022 Q59.9619 688.648 59.1344 685.497 Q58.3069 682.314 56.6518 679.195 L62.1899 679.195 Q63.5267 682.346 64.227 685.656 Q64.9272 688.966 64.9272 692.372 Q64.9272 700.902 59.9619 705.899 Q54.9967 710.864 46.5303 710.864 Q37.7774 710.864 32.6531 706.154 Q27.4968 701.411 27.4968 693.391 Q27.4968 686.197 32.1438 682.028 Q36.7589 677.826 44.7161 677.826 M42.9973 683.683 Q38.1912 683.747 35.3266 686.388 Q32.4621 688.998 32.4621 693.327 Q32.4621 698.229 35.2312 701.189 Q38.0002 704.117 43.0292 704.562 L42.9973 683.683 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M44.7161 637.722 L47.5806 637.722 L47.5806 664.649 Q53.6281 664.267 56.8109 661.021 Q59.9619 657.743 59.9619 651.918 Q59.9619 648.544 59.1344 645.393 Q58.3069 642.21 56.6518 639.091 L62.1899 639.091 Q63.5267 642.242 64.227 645.552 Q64.9272 648.862 64.9272 652.268 Q64.9272 660.798 59.9619 665.795 Q54.9967 670.76 46.5303 670.76 Q37.7774 670.76 32.6531 666.05 Q27.4968 661.307 27.4968 653.287 Q27.4968 646.093 32.1438 641.924 Q36.7589 637.722 44.7161 637.722 M42.9973 643.579 Q38.1912 643.643 35.3266 646.284 Q32.4621 648.894 32.4621 653.223 Q32.4621 658.125 35.2312 661.085 Q38.0002 664.013 43.0292 664.458 L42.9973 643.579 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M33.7671 604.653 L14.479 604.653 L14.479 598.796 L64.0042 598.796 L64.0042 604.653 L58.657 604.653 Q61.8398 606.499 63.3994 609.331 Q64.9272 612.132 64.9272 616.079 Q64.9272 622.54 59.771 626.614 Q54.6147 630.657 46.212 630.657 Q37.8093 630.657 32.6531 626.614 Q27.4968 622.54 27.4968 616.079 Q27.4968 612.132 29.0564 609.331 Q30.5842 606.499 33.7671 604.653 M46.212 624.609 Q52.6732 624.609 56.3653 621.967 Q60.0256 619.294 60.0256 614.647 Q60.0256 610 56.3653 607.326 Q52.6732 604.653 46.212 604.653 Q39.7508 604.653 36.0905 607.326 Q32.3984 610 32.3984 614.647 Q32.3984 619.294 36.0905 621.967 Q39.7508 624.609 46.212 624.609 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M49.9359 587.338 L28.3562 587.338 L28.3562 581.481 L49.7131 581.481 Q54.7739 581.481 57.3202 579.508 Q59.8346 577.535 59.8346 573.588 Q59.8346 568.846 56.8109 566.108 Q53.7872 563.339 48.5673 563.339 L28.3562 563.339 L28.3562 557.483 L64.0042 557.483 L64.0042 563.339 L58.5296 563.339 Q61.7762 565.472 63.3676 568.304 Q64.9272 571.105 64.9272 574.829 Q64.9272 580.972 61.1078 584.155 Q57.2883 587.338 49.9359 587.338 M27.4968 572.601 L27.4968 572.601 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M58.657 539.754 L77.5631 539.754 L77.5631 545.643 L28.3562 545.643 L28.3562 539.754 L33.7671 539.754 Q30.5842 537.908 29.0564 535.107 Q27.4968 532.275 27.4968 528.36 Q27.4968 521.867 32.6531 517.824 Q37.8093 513.75 46.212 513.75 Q54.6147 513.75 59.771 517.824 Q64.9272 521.867 64.9272 528.36 Q64.9272 532.275 63.3994 535.107 Q61.8398 537.908 58.657 539.754 M46.212 519.83 Q39.7508 519.83 36.0905 522.503 Q32.3984 525.145 32.3984 529.792 Q32.3984 534.439 36.0905 537.112 Q39.7508 539.754 46.212 539.754 Q52.6732 539.754 56.3653 537.112 Q60.0256 534.439 60.0256 529.792 Q60.0256 525.145 56.3653 522.503 Q52.6732 519.83 46.212 519.83 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip542)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"310.553,1372.3 429.468,1301.96 590.826,1207.86 1126.85,713.803 1501.51,291.424 1662.87,158.798 2037.53,89.3612 2198.89,85.838 2293.27,95.2497 \"/>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M2020.1 196.379 L2282.7 196.379 L2282.7 92.6992 L2020.1 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2020.1,196.379 2282.7,196.379 2282.7,92.6992 2020.1,92.6992 2020.1,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip540)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2043.45,144.539 2183.57,144.539 \"/>\n",
       "<path clip-path=\"url(#clip540)\" d=\"M2220.76 164.227 Q2218.95 168.856 2217.24 170.268 Q2215.53 171.68 2212.66 171.68 L2209.26 171.68 L2209.26 168.115 L2211.76 168.115 Q2213.51 168.115 2214.49 167.282 Q2215.46 166.449 2216.64 163.347 L2217.4 161.403 L2206.92 135.893 L2211.43 135.893 L2219.53 156.171 L2227.63 135.893 L2232.15 135.893 L2220.76 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip540)\" d=\"M2239.44 157.884 L2247.08 157.884 L2247.08 131.518 L2238.77 133.185 L2238.77 128.926 L2247.03 127.259 L2251.71 127.259 L2251.71 157.884 L2259.35 157.884 L2259.35 161.819 L2239.44 161.819 L2239.44 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(mat_size,tcpu./tgpu, yaxis= :log, xaxis=:log, ylabel=\"GPU speedup\", xlabel=\"matrix size\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2: Computing Custom Gradients on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider the challenge of moving from a CPU-array-based implementation to a GPU-based-implementation. In [this paper](https://arxiv.org/pdf/2406.13191), we needed to efficiently compute the gradient of the following expression:\n",
    "\n",
    "$\\mathcal{L}=-\\left\\Vert c^{T}+\\lambda^{T}A+\\mu^{T}C\\right\\Vert _{1}+\\lambda^{T}b+\\mu^{T}d.$\n",
    "\n",
    "with respect to the dual variables $\\lambda$ and $\\mu$. These gradients are given by\n",
    "* $\\frac{\\partial\\mathcal{L}}{\\partial\\lambda} =b-C{\\rm sign}\\left(c^{T}+\\lambda^{T}A+\\mu^{T}C\\right)$\n",
    "* $\\frac{\\partial\\mathcal{L}}{\\partial\\mu} =d-A{\\rm sign}\\left(c^{T}+\\lambda^{T}A+\\mu^{T}C\\right)$\n",
    "\n",
    "Highly optimzied, non-allocating CPU code looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_gradient (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function obj_gradient(lambda::Float64, mu::Vector{Float64}, grad_lambda::Float64, grad_mu::Vector{Float64}, A::Vector{Float64}, b::Vector{Float64}, C::Matrix{Float64}, d::Vector{Float64}, cg1::Vector{Float64}, ng::Int64, nl::Int64)\n",
    "    grad_lambda  = copy(b[1])\n",
    "    grad_mu     .= copy.(d)\n",
    "\n",
    "    for el in 1:ng\n",
    "        cv = @view C[:,el]\n",
    "        s_gamma = Float64(sign(cg1[el] + lambda*A[el] + dot(mu, cv)))\n",
    "\n",
    "        grad_lambda += -s_gamma*A[el]\n",
    "        for jj = 1:2*nl\n",
    "            grad_mu[jj] += -s_gamma*C[jj,el]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return grad_lambda, grad_mu\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this code with synthetic power system data, assuming 10,000 lines and 5,000 generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl          = 10000\n",
    "ng          = 5000\n",
    "lambda      = 0.0\n",
    "grad_lambda = 0.0\n",
    "mu          = randn(2*nl)\n",
    "grad_mu     = randn(2*nl)\n",
    "A           = randn(ng)\n",
    "b           = randn(1)\n",
    "C           = randn(2*nl,ng)\n",
    "d           = rand(2*nl)\n",
    "cg1         = rand(ng)\n",
    "\n",
    "cpu_gradient_time = @belapsed obj_gradient(lambda, mu, grad_lambda, grad_mu, A, b, C, d, cg1, ng, nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is **not efficient** with CuArrays! Why not? **Scalar indexing.** Generally, scalar indexing on GPUs is a bad idea. Recently, CUDA.jl has introduced ```UnifiedMemory```, where data accessed from the GPU device can be operated on via CPU with just ~2x latency, but this is for specialty applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\nIf you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\nto enable scalar iteration globally or for the operations in question.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore should be avoided.\n",
      "\n",
      "If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n",
      "to enable scalar iteration globally or for the operations in question.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base .\\error.jl:35\n",
      " [2] errorscalar(op::String)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:155\n",
      " [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:128\n",
      " [4] assertscalar(op::String)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:116\n",
      " [5] getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:50 [inlined]\n",
      " [6] scalar_getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:36 [inlined]\n",
      " [7] _getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:19 [inlined]\n",
      " [8] getindex(::CuArray{Float64, 2, CUDA.DeviceMemory}, ::Int64, ::Int64)\n",
      "   @ GPUArrays C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:17\n",
      " [9] top-level scope\n",
      "   @ c:\\Users\\chev8\\.julia\\dev\\ParallelNotes\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y104sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "x = CuArray(randn(10,10))\n",
    "x[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to make this code efficient, we had to reformulate the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_gradient_gpu_friendly (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function obj_gradient_gpu_friendly(lambda::Float64, mu::CuVector{Float64}, grad_lambda::Float64, grad_mu::CuVector{Float64}, a::CuVector{Float64}, b::Vector{Float64}, c::CuMatrix{Float64}, d::CuVector{Float64}, cg1::CuVector{Float64}, sign_gamma::CuVector{Float64}, ng::Int64, nl::Int64, ct::CuMatrix{Float64})\n",
    "    grad_lambda  = copy(b[1])\n",
    "    # the next two lines do this: sign_gamma .= sign.(cg1 .+ lambda.*a .+ c'*mu)\n",
    "    mul!(sign_gamma,ct,mu)\n",
    "    sign_gamma  .= sign.(cg1 .+ lambda.*a .+ sign_gamma)\n",
    "    grad_lambda -= dot(sign_gamma,a)\n",
    "    mul!(grad_mu,c,sign_gamma)\n",
    "    grad_mu .= .-grad_mu\n",
    "    grad_mu .+= d\n",
    "\n",
    "    return grad_lambda, grad_mu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0067429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl          = 10000\n",
    "ng          = 5000\n",
    "lambda      = 0.0\n",
    "grad_lambda = 0.0\n",
    "mu          = CuVector(randn(2*nl))\n",
    "grad_mu     = CuVector(randn(2*nl))\n",
    "A           = CuVector(randn(ng))\n",
    "b           = randn(1)\n",
    "C           = CuMatrix(randn(2*nl,ng))\n",
    "Ct          = copy(C')\n",
    "d           = CuVector(rand(2*nl))\n",
    "cg1         = CuVector(rand(ng))\n",
    "sign_gamma  = CuVector(zeros(ng))\n",
    "\n",
    "gpu_gradient_time = @belapsed CUDA.@sync obj_gradient_gpu_friendly(lambda, mu, grad_lambda, grad_mu, A, b, C, d, cg1, sign_gamma, ng, nl, Ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computation speedup: 13.33550846075131\n"
     ]
    }
   ],
   "source": [
    "println(\"Gradient computation speedup: \", cpu_gradient_time/gpu_gradient_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```The main lesson here is this: performant Julia code, which was written with scalar indexing to avoid memory allocation or for the sake of readability/understandability, will often need to be reformulated to run efficiently on the GPU.```\n",
    "* GPU arrays can be used to solve most of your problems, however, and should be used how typical abstract arrays are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, it is worth noting that CUDA has some awesome sparse, and non-sparse, linear system solver tools.\n",
    "* ```CUBLAS.getrf_batched!``` will **batch** LU factorize dense matrices on the GPU in parallel (with or without pivoting)\n",
    "* ```UBLAS.trsm_batched!``` will **batch** solve the resulting systems $Ly=b$; $Ux=y$\n",
    "* example application is in our recent [paper](https://arxiv.org/pdf/2311.11833)\n",
    "* Here is the LU solve function we wrote/used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lu_solve!(A::Array{CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, 1}, b::Array{CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, 1})\n",
    "    # solve Ax = LUx = b\n",
    "    # \n",
    "    # stage 1: solve \"Ly = b\"\n",
    "    side  = 'L'   # left side\n",
    "    ul    = 'L'   # solve \"L\"\n",
    "    tA    = 'N'   # not transposed\n",
    "    dA    = 'U'   # assume all ones on diagonal!\n",
    "    alpha = 1.0   # scalar\n",
    "    CUBLAS.trsm_batched!(side, ul, tA, dA, alpha, A, b);\n",
    "\n",
    "    # solve u\n",
    "    ul    = 'U'  # solve \"U\"\n",
    "    dA    = 'N'  # read diagonal\n",
    "    CUBLAS.trsm_batched!(side, ul, tA, dA, alpha, A, b);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. GPU Kernels ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While CuArray and the CUDA linear algebra libraries are very convenient and quite fast, custom GPU kernels can be used to achieve faster speeds still. A GPU kernel is \"a function that runs in parallel on a GPU\". It isn't like a custom function, however, since it returns \"nothing\"; instead, it mutates the given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```function!(x,y) -> nothing```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you launch your own Kernels, you get to **personally** manage the thread and block computations. Joy! This gives a high degree of computational granularity. Don't do this unless you are willing to invest the time in (1) writing good, safe code and (2) profiling against simpler methods to ensure there is actually a gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excellent overview of GPU threads/blocks is given [here](https://cuda.juliagpu.org/stable/tutorials/introduction/). Essentially, the kernel uses a **trick** to determing its own computational payload. In summary, this trick is based on these funcitons:\n",
    "1. ```threadIdx()``` calls the thread ID on a block\n",
    "2. ```blockDim()``` returns the dimension of the block\n",
    "3. ```blockIdx()``` returns the index of the block\n",
    "\n",
    "These expression can be queried in 3 dimensions (x,y,z). Blocks are managed and executed by a \"streaming multiprocessor\" (SM).\n",
    "\n",
    "![gpu](./figs/gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image comes from an overview of CUDA's [C/C++ library](https://developer.nvidia.com/blog/even-easier-introduction-cuda/). While Cu/GPU arrays are unique to the Julia API, block/thread management is a generic skill (useful in Julia or C/C++)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: when working with GPU kernels, scalar indexing (calling ```v[i]```) ~~is really bad~~ **is good and necessary!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: Simple Cuda Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $m$ quadratic systems, each of whose energy we can compute individually as $e_{i}=x_{i}^{T}A_{i}x_{i}$. We can compute these in parallel using GPU kernels. On the cpu, the code is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_cpu! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_cpu!(A,x,e)\n",
    "    for i in 1:length(e)\n",
    "        e[i] = x[i]'*A[i]*x[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036418"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "A = [randn(n_states,n_states) for ii in 1:n_particles]\n",
    "x = [randn(n_states) for ii in 1:n_particles]\n",
    "e = [randn() for ii in 1:n_particles]\n",
    "\n",
    "t_cpu = @belapsed quadratic_computation_cpu!(A,x,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a gpu kernel to accomplish the same task. Note: the kernel is launched by calling ```@cuda``` and then passing the optional keywords \n",
    "* ```threads```, to specify the number of threads that should launch, and\n",
    "* ```blocks```, to specify the number of blocks\n",
    "\n",
    "First, we define a kernel function. Note:\n",
    "* it returns nothing\n",
    "* it directly mutates the inputs\n",
    "* it uses scalar indexing to access devise memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu!(A,x,e,n_states)\n",
    "    i = threadIdx().x\n",
    "    for jj in 1:n_states # note -- row major!\n",
    "        for ii in 1:n_states\n",
    "            @inbounds e[i] += sum(x[ii,i]*A[ii,jj,i]*x[jj,i])\n",
    "        end\n",
    "    end\n",
    "    return # returns nothing :)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **trick** is this: the tiny little thread only knows two things: its name (i.e., its index), and how to do some very basic math. It uses its name (i.e., its numerical index) to decide which math it should do.\n",
    "\n",
    "Let's specify the number of threads we want to use, and then call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Number of threads in x-dimension exceeds device limit (40960 > 1024).",
     "output_type": "error",
     "traceback": [
      "Number of threads in x-dimension exceeds device limit (40960 > 1024).\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base .\\error.jl:35\n",
      "  [2] diagnose_launch_failure(f::CuFunction, err::CuError; blockdim::CuDim3, threaddim::CuDim3, shmem::Int64)\n",
      "    @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:97\n",
      "  [3] launch(::CuFunction, ::CUDA.KernelState, ::CuDeviceArray{Float64, 3, 1}, ::CuDeviceMatrix{Float64, 1}, ::CuDeviceVector{Float64, 1}, ::Int64; blocks::Int64, threads::Int64, cooperative::Bool, shmem::Int64, stream::CuStream)\n",
      "    @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:73\n",
      "  [4] launch\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:52 [inlined]\n",
      "  [5] #972\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:189 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:149 [inlined]\n",
      "  [7] macro expansion\n",
      "    @ .\\none:0 [inlined]\n",
      "  [8] convert_arguments\n",
      "    @ .\\none:0 [inlined]\n",
      "  [9] #cudacall#971\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:191 [inlined]\n",
      " [10] cudacall\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:187 [inlined]\n",
      " [11] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\compiler\\execution.jl:279 [inlined]\n",
      " [12] macro expansion\n",
      "    @ .\\none:0 [inlined]\n",
      " [13] #_#1157\n",
      "    @ .\\none:0 [inlined]\n",
      " [14] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\compiler\\execution.jl:114 [inlined]\n",
      " [15] var\"##core#608\"()\n",
      "    @ Main C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:561\n",
      " [16] var\"##sample#609\"(::Tuple{}, __params::BenchmarkTools.Parameters)\n",
      "    @ Main C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:570\n",
      " [17] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; maxevals::Int64, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:187\n",
      " [18] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters)\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:182\n",
      " [19] #invokelatest#2\n",
      "    @ .\\essentials.jl:1055 [inlined]\n",
      " [20] invokelatest\n",
      "    @ .\\essentials.jl:1052 [inlined]\n",
      " [21] #lineartrial#46\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:51 [inlined]\n",
      " [22] lineartrial\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:50 [inlined]\n",
      " [23] tune!(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, verbose::Bool, pad::String, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:300\n",
      " [24] tune!\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:289 [inlined]\n",
      " [25] tune!(b::BenchmarkTools.Benchmark)\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:289\n",
      " [26] top-level scope\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:447"
     ]
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "@belapsed @cuda threads=length(e) quadratic_computation_gpu!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OH NOOOO!** We tried to launch too many threads -- we only have 1024 on my GPU. We could either\n",
    "1. 40x the amout of work each thread has to do, or\n",
    "2. ask for additional blocks\n",
    "\n",
    "Let's write both. Here is an updated function for option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu_overload! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu_overload!(A,x,e,n_states)\n",
    "    i = threadIdx().x\n",
    "    for jj in 1:n_states # note -- row major!\n",
    "        for ii in 1:n_states\n",
    "            for kk in 0:39\n",
    "                @inbounds e[i + kk*1024] += sum(x[ii,i + kk*1024]*A[ii,jj,i + kk*1024]*x[jj,i + kk*1024])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0069283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "@belapsed CUDA.@sync @cuda threads=1024 quadratic_computation_gpu_overload!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to good -- we didn't exploit parallelism very effectively, because we overloaded those poor threads.\n",
    "\n",
    "Here is how we can apply option 2, where we deploy blocks with additional threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu_blocks! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu_blocks!(A,x,e,n_states)\n",
    "    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    # if needed, we can also define a stride to loop over larger vectors -- not used here\n",
    "    # stride = gridDim().x * blockDim().x\n",
    "\n",
    "    @inbounds for ii in 1:n_states # note -- row major!\n",
    "        @inbounds for jj in 1:n_states\n",
    "            @inbounds e[index] += x[ii,index]*A[ii,jj,index]*x[jj,index]\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "numblocks   = 40\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "t_gpu = @belapsed CUDA.@sync @cuda threads=1024 blocks=numblocks quadratic_computation_gpu_blocks!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are at least 50% faster than the CPU, but we can be much faster still if we exploit multi-dimensional multithreading (x,y,z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-CUDA kernels\n",
    "For non-CUDA kernels, Julia has [KernelAbstractions.jl](https://juliagpu.github.io/KernelAbstractions.jl/stable/examples/matmul/). Here is an [excellent video](https://www.youtube.com/watch?v=1Q5Hpwu3tpo&t=3590s), too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2: An ADMM example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of us are familiar with ADMM, a distributed optimization technique which iteratively minimizes a set of distributed primal objectives, takes a dual ascent step, and continues on. In a [recent paper](https://arxiv.org/pdf/2310.09410) by Ryu et al., they solve the following problem over $|S|$ subsystems:\n",
    "\n",
    "![s](.\\figs\\admm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They forumularte a Lagrange dual, and then apply stationarity conditions to explictly compute a solution for the primal as a function of the duals:\n",
    "\n",
    "![s](.\\figs\\admm_soln.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update algorithm is given below:\n",
    "\n",
    "![s](.\\figs\\alg.png)\n",
    "\n",
    "Given the following primal and dual update rules, **can you write a kernel?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here is the CPU code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS     = 100 # number of systems\n",
    "nv     = 50  # number of variables\n",
    "C      = randn(nv)\n",
    "x      = [randn(nv)    for S in 1:nS]\n",
    "lambda = [randn(nv)    for S in 1:nS]\n",
    "xhat   = [randn(nv)    for S in 1:nS]\n",
    "B      = [randn(nv,nv) for S in 1:nS]\n",
    "xupper = [10*rand(nv)  for S in 1:nS]\n",
    "xlower = [-10*rand(nv) for S in 1:nS]\n",
    "\n",
    "for S in 1:nS\n",
    "\n",
    "    # x update\n",
    "    xhat[S] = (-1)*(C + sum(B[S]'*lambda[S] for S in 1:nS) - sum(x')')\n",
    "    x[S] = min.( max.(xhat[S], xlower[S]), xupper[S])\n",
    "    \n",
    "    # update lambda\n",
    "    lambda[S] = lambda[S] + p*(B[S]*x[S]-x[S])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function admm_kernel(C, x, xhat, lambda, xupper, xlower)\n",
    "\n",
    "    return: nothing\n",
    "end\n",
    "\n",
    "# call the kernel!\n",
    "\n",
    "@cude threads blocks admm_kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
