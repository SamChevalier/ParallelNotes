{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParallelNotes.jl #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome! Appologies to Amrit for the notebook form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\chev8\\.julia\\dev\\ParallelNotes`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") # activate this project -- that's all this does\n",
    "\n",
    "# this will call the packages and APIs we need!!\n",
    "using LinearAlgebra, CUDA, Hwloc, BenchmarkTools, JuMP, Ipopt, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will teach us something abour parallel computing in\t[Julia](https://julialang.org/benchmarks/). We won't jump right into GPU programming, however, since parallelization on the CPU should always be explored before GPU programming is exploited. **Crawl, walk, run, FLY!**\n",
    "\n",
    "So, let's start by asking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Should I parallelize my code?\"**\n",
    "\t\n",
    "* **Does performance matter && does your problem naturally parallelize??**\n",
    "    * *No*: Probably no need/not possible. **END**.\n",
    "    * *Yes*: Let's see what we can do!\n",
    "        * **Do you need massive parallelization || can functions run on GPU?**\n",
    "            * *Either No*: Just use CPU threads. **END**.\n",
    "            * *Both Yes*: Use the GPU!\n",
    "                * **Are you willing to sacrifice a little speed for a lot of conveniencesimplicity?**\n",
    "                    * *Yes*: Use Cu/ROC/one/Mtl Arrays + LinAlg APIs. **END**.\n",
    "                    * *No*: Use GPU Kernels.\n",
    "                        * **Is your code Pure NVIDIA?**\n",
    "                            * *Yes*: Just use the @cuda kernal macro.\n",
    "                            * *No*: Use KernelAbstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Julia specific tutorial, but its premise is universal. If you are coming from C++, good news: [\"writing kernels in Julia is very similar to writing kernels in CUDA C/C++\"](https://cuda.juliagpu.org/stable/development/kernel/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a quick tl;dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to write GPU accelerated code, here is Sam's suggestion:\n",
    "1. write highly optimized CPU code (minimize run-time memory allocations, ensure the code is \"type stable\", exploit optimized BLAS routines, etc), **then**\n",
    "2. parallelize the CPU code, testing for speedup and correctness, **then**\n",
    "3. move the the GPU, exploiting GPU arrays or custom kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to write optimized GPU code you need to\n",
    "1. have an impeccable benchmark on the CPU first\n",
    "2. be obsessed with timing statistics and memory allocation\n",
    "3. realize that it will take a lot of tinkering before it's optimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three levels of GPU utilization: applications (e.g., Flux, or DiffEqs), arrays, and kernels. This is summarized by Time Besard in the following figures:\n",
    "![app](./figs/apps.png)\n",
    "![arrays](./figs/arrays.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. CPU Parallelization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try some CPU parallelization exercises. Let's see how many CPU threads we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Available CPU Threads: 20. Physical Threads: 14\n",
      "Julia is only using this many threads: 6\n"
     ]
    }
   ],
   "source": [
    "println(\"Total Available CPU Threads: \", Sys.CPU_THREADS, \". Physical Threads: \", num_physical_cores())\n",
    "println(\"Julia is only using this many threads: \", Threads.nthreads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without telling Julia to multi-thread, it will perform opperations on a single thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for ii in 1:48\n",
    "\tprintln(Threads.threadid())\n",
    "\tsleep(0.1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the ```Threads.@threads``` macro to parallelize the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "5\n",
      "12\n",
      "11\n",
      "8\n",
      "3\n",
      "10\n",
      "9\n",
      "7\n",
      "2\n",
      "4\n",
      "12\n",
      "5\n",
      "3\n",
      "11\n",
      "6\n",
      "8\n",
      "2\n",
      "7\n",
      "9\n",
      "10\n",
      "4\n",
      "3\n",
      "6\n",
      "7\n",
      "5\n",
      "11\n",
      "8\n",
      "12\n",
      "2\n",
      "10\n",
      "4\n",
      "9\n",
      "11\n",
      "7\n",
      "10\n",
      "6\n",
      "8\n",
      "12\n",
      "4\n",
      "3\n",
      "5\n",
      "9\n",
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "Threads.@threads for ii in 1:48\n",
    "\tprintln(Threads.threadid())\n",
    "\tsleep(0.1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1: CPU Parallelized Matrix Vector Products ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to compute $m$ matrix vector products $y=Ax$, i.e., $y\\in{\\mathbb R}^{n \\times m}$, $A\\in{\\mathbb R}^{n \\times n}$, and $n\\in{\\mathbb R}^{n \\times m}$. Without parallelization, you might just compute the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  $y_1 = Ax_1$\n",
    "*  $y_2 = Ax_2$\n",
    "*  ...\n",
    "*  $y_m = Ax_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvp_cpu_serial (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick note: Matrix{Float64} == Array{Float64,2}\n",
    "function mvp_cpu_serial(A::Matrix{Float64}, x::Array{Float64,2}, y::Array{Float64,2})\n",
    "\tfor ii in 1:size(x,2)\n",
    "\t\ty[:,ii] .= A*x[:,ii] # note! \"mul!\" would be slightly faster\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.020901 seconds (300 allocations: 787.500 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0043045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_mvps = 50;\n",
    "A = randn(1000,1000);\n",
    "x = randn(1000,num_mvps);\n",
    "y = randn(1000,num_mvps);\n",
    "@time mvp_cpu_serial(A,x,y) # quick and dirty\n",
    "time_serial = @belapsed mvp_cpu_serial($A,$x,$y) # much more percise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the CPU to parallelize! We can use the `Threads@threads` macros to tell Julia to **parallelize** the operation, i.e., to chop up the taks and assign it to available CPU threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvp_cpu_parallel (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mvp_cpu_parallel(A::Matrix{Float64}, x::Array{Float64,2}, y::Array{Float64,2})\n",
    "\tThreads.@threads for ii in 1:size(x,2)\n",
    "\t\ty[:,ii] .= A*x[:,ii] # note! \"mul!\" would be slightly faster\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.003226 seconds (362 allocations: 795.156 KiB)\n",
      "Parallel vs serial speedup: 2.242160641733514\n"
     ]
    }
   ],
   "source": [
    "num_mvps = 50;\n",
    "A = randn(1000,1000);\n",
    "x = randn(1000,num_mvps);\n",
    "y = randn(1000,num_mvps);\n",
    "@time mvp_cpu_parallel(A,x,y) # quick and dirty\n",
    "time_parallel = @belapsed mvp_cpu_parallel($A,$x,$y) # much more percise!\n",
    "\n",
    "println(\"Parallel vs serial speedup: \", time_serial/time_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2: CPU Parallelized Optimization ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have $m$ optimization problems we need to solve. For example, in the GO competition, I wanted to parallelize across power flows solutions in space, and generator feasibility over time (startup/shutdown/ramping constraints), as shown in Fig. 2 of [this paper.](https://arxiv.org/pdf/2310.06650): ![parallel](./figs/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pose $m$ parallelizable optimization problems as the followin $\\min\\;x_{i}^{T}M_{i}x_{i},\\;{\\rm s.t.}\\,A_{i}x_{i}\\le b_{i}$ (where $M_i$ is PSD, don't worry :). That is, we want to solve\n",
    "*  $\\min\\;x_{1}^{T}M_{1}x_{1},\\;{\\rm s.t.}\\,A_{1}x_{1}\\le b_{1}$\n",
    "*  $\\min\\;x_{2}^{T}M_{2}x_{2},\\;{\\rm s.t.}\\,A_{2}x_{2}\\le b_{2}$\n",
    "* ...\n",
    "*  $\\min\\;x_{m}^{T}M_{m}x_{m},\\;{\\rm s.t.}\\,A_{m}x_{m}\\le b_{m}$,\n",
    "\n",
    "where each of these problems can be solved in parallel. Let's solve these in series, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve_opt_serial (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function opt(M,A,b)\n",
    "    nv = length(b)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, x[1:nv])\n",
    "    @constraint(model, A*x .<= b)\n",
    "    @objective(model, Min, x'*M*x)\n",
    "    optimize!(model)\n",
    "    return objective_value(model)\n",
    "end\n",
    "\n",
    "function solve_opts(M,A,b;serial=true)\n",
    "    n_opt = length(M)\n",
    "    if serial == true\n",
    "        for ii in 1:n_opt\n",
    "            opt(M[ii],A[ii],b[ii])\n",
    "        end\n",
    "    else\n",
    "        Threads.@threads for ii in 1:n_opt\n",
    "            opt(M[ii],A[ii],b[ii])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the parallel optimization problem, and test the serial vs parallel implementations (see ```run_opt.jl``` -- the notebook implementation was giving me some trouble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. GPU Parallelization via GPU Arrays ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU programming is actually very eary! Via GPUArrays (which you never actually need to use, since it's a backend), Julia users can define arrays which live on GPU device memory. A wide set of operations applied to these arrays will automatically run directly on the GPU. As summarized [here](https://enccs.github.io/julia-for-hpc/GPU/), we use the following:\n",
    "* CUDA.jl for NVIDIA GPUs\n",
    "* AMDGPU.jl for AMD GPUs\n",
    "* oneAPI.jl for Intel GPUs\n",
    "* Metal.jl for Apple M-series GPUs\n",
    "\n",
    "Moving from a CPU-based array to a GPU-based array, we simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×50 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " -0.675117     0.295147    0.132048   …  -0.613032    0.878265    0.868063\n",
       "  0.596794     0.943871    0.997975      -0.314947    0.381253    0.359303\n",
       " -0.461238    -1.35036     0.858579      -0.969266   -0.941378    0.228403\n",
       "  0.699146    -0.035051   -0.28417        0.191442   -0.50236    -1.45239\n",
       "  1.15447     -1.11356    -0.0819285     -0.880906   -1.1362     -0.576536\n",
       " -0.687645    -0.173546   -1.47307    …   1.65144    -1.93518    -0.643768\n",
       " -1.06653      2.49486    -0.846289       1.39637     0.203339   -2.40492\n",
       "  0.257953     0.771995   -0.703725       1.4013     -1.1984     -2.35125\n",
       " -1.91525      0.509783   -0.0964984     -1.06955    -0.0440029  -0.127668\n",
       " -0.109453     0.75792     0.219472      -0.328597   -1.14968     1.17926\n",
       "  ⋮                                   ⋱                          \n",
       " -0.166979     0.559219    1.01956        0.361106    0.232901   -1.0674\n",
       " -0.328493     3.07104    -0.453717      -0.736967    0.548327   -0.176992\n",
       " -0.00176071   0.202827    1.48086       -0.370103   -0.951492   -1.44647\n",
       " -1.24923      0.668496    1.52608        0.0546132  -0.734767   -0.753611\n",
       " -1.96931      0.412851   -0.516362   …  -0.420781   -1.05564    -0.170836\n",
       "  0.48497     -0.0157903   1.04815       -0.861307    0.428855   -1.30258\n",
       " -0.159557    -0.0933063   0.134319      -0.720338   -0.685523   -0.283115\n",
       " -0.861603     2.78762    -0.329502      -1.23709    -1.13172    -0.196613\n",
       "  0.363295     0.249008   -0.768695       0.13416     3.10301     1.96952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cpu = randn(100)\n",
    "x_gpu = CuArray(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×50 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 0.455783   0.0871119    0.0174367   …  0.375809   0.77135     0.753534\n",
       " 0.356163   0.890892     0.995954       0.0991919  0.145354    0.129099\n",
       " 0.212741   1.82346      0.737158       0.939477   0.886193    0.0521682\n",
       " 0.488805   0.00122858   0.0807526      0.0366501  0.252366    2.10944\n",
       " 1.33279    1.24002      0.00671228     0.775995   1.29094     0.332394\n",
       " 0.472855   0.0301181    2.16993     …  2.72727    3.74491     0.414438\n",
       " 1.13749    6.22432      0.716205       1.94985    0.0413468   5.78362\n",
       " 0.0665397  0.595976     0.495229       1.96363    1.43617     5.52836\n",
       " 3.66818    0.259879     0.00931193     1.14393    0.00193625  0.016299\n",
       " 0.01198    0.574443     0.048168       0.107976   1.32177     1.39065\n",
       " ⋮                                   ⋱                         \n",
       " 0.027882   0.312726     1.0395         0.130397   0.0542428   1.13934\n",
       " 0.107908   9.4313       0.205859       0.54312    0.300663    0.031326\n",
       " 3.1001e-6  0.0411388    2.19294        0.136976   0.905337    2.09229\n",
       " 1.56058    0.446887     2.32892        0.0029826  0.539883    0.56793\n",
       " 3.87818    0.170446     0.26663     …  0.177056   1.11437     0.029185\n",
       " 0.235196   0.000249335  1.09861        0.741849   0.183917    1.69671\n",
       " 0.0254584  0.00870606   0.0180415      0.518887   0.469942    0.0801544\n",
       " 0.742359   7.77085      0.108572       1.53039    1.28078     0.0386567\n",
       " 0.131984   0.0620051    0.590892       0.017999   9.62867     3.87901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = x_gpu.^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, you just ran your **first GPU calculations** in Julia! It was that easy! So, how much computation can you push to the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfGPUMemoryError",
     "evalue": "Out of GPU memory trying to allocate 74.506 GiB\nEffective GPU memory usage: 23.53% (1.881 GiB/7.996 GiB)\nMemory pool usage: 771.332 MiB (800.000 MiB reserved)\n",
     "output_type": "error",
     "traceback": [
      "Out of GPU memory trying to allocate 74.506 GiB\n",
      "Effective GPU memory usage: 23.53% (1.881 GiB/7.996 GiB)\n",
      "Memory pool usage: 771.332 MiB (800.000 MiB reserved)\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] _pool_alloc\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:660 [inlined]\n",
      " [2] macro expansion\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:617 [inlined]\n",
      " [3] macro expansion\n",
      "   @ .\\timing.jl:421 [inlined]\n",
      " [4] pool_alloc\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\memory.jl:616 [inlined]\n",
      " [5] CuArray{Int64, 2, CUDA.DeviceMemory}(::UndefInitializer, dims::Tuple{Int64, Int64})\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:74\n",
      " [6] CuArray\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:128 [inlined]\n",
      " [7] (CuArray{Int64})(::UndefInitializer, ::Int64, ::Int64)\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:146\n",
      " [8] ones(::Type, ::Int64, ::Vararg{Int64})\n",
      "   @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\array.jl:772\n",
      " [9] top-level scope\n",
      "   @ c:\\Users\\chev8\\.julia\\dev\\ParallelNotes\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sZmlsZQ==.jl:8"
     ]
    }
   ],
   "source": [
    "n1 = 1000\n",
    "v1 = CUDA.ones(Int, n1,n1)\n",
    "\n",
    "n2 = 10000\n",
    "v2 = CUDA.ones(Int, n2,n2)\n",
    "\n",
    "n3 = 100000\n",
    "v3 = CUDA.ones(Int, n3,n3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1: CuArrays for matrix-matrix products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a series of matrix-matirx + broadcasted scalar addition computaions. These examples were chosen to highlight GPU efficiency. We run the computations on the CPU and GPU and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat_mat_product_cpu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function mat_mat_product_cpu(A::Matrix{Float32},x::Matrix{Float32},y::Matrix{Float32})\n",
    "    b  = Float32(1)\n",
    "    y .= A*x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat_mat_product_gpu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mat_mat_product_gpu(A::CuArray{Float32, 2},x::CuArray{Float32, 2},y::CuArray{Float32, 2})\n",
    "    b  = Float32(1)\n",
    "    y .= A*x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.0010050251256281408, 0.0031796682282119178, 0.005437229437229437, 1.1921824104234529, 9.832268370607029, 388.58198614318707, 876.0830490879431, 1554.4147756670855, 1994.197735663303]\n"
     ]
    }
   ],
   "source": [
    "mat_size = [3; 5; 10; 100; 500; 1000; 5000; 10000; 15000]\n",
    "tcpu = zeros(length(mat_size))\n",
    "tgpu = zeros(length(mat_size))\n",
    "A = []\n",
    "x = []\n",
    "y = []\n",
    "ii = 1\n",
    "\n",
    "for mm in mat_size\n",
    "    A = Float32.(randn(mm,mm))\n",
    "    x = Float32.(randn(mm,mm))\n",
    "    y = Float32.(randn(mm,mm))\n",
    "    tcpu[ii] = @belapsed mat_mat_product_cpu(A,x,y)\n",
    "\n",
    "    A = CuArray(Float32.(randn(mm,mm)))\n",
    "    x = CuArray(Float32.(randn(mm,50)))\n",
    "    y = CuArray(Float32.(randn(mm,50)))\n",
    "    tgpu[ii] = @belapsed CUDA.@sync mat_mat_product_gpu(A,x,y)\n",
    "    ii += 1\n",
    "    println()\n",
    "end\n",
    "\n",
    "println(tcpu./tgpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the result, in log-log scale, to show GPU speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip010\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip011\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M251.071 1410.9 L2352.76 1410.9 L2352.76 47.2441 L251.071 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip012\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2103\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1126.85,1410.9 1126.85,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2198.89,1410.9 2198.89,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,1168.47 2352.76,1168.47 \"/>\n",
       "<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,759.911 2352.76,759.911 \"/>\n",
       "<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,351.353 2352.76,351.353 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1126.85,1410.9 1126.85,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2198.89,1410.9 2198.89,1392 \"/>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M1092.62 1485.02 L1100.26 1485.02 L1100.26 1458.66 L1091.95 1460.32 L1091.95 1456.06 L1100.21 1454.4 L1104.89 1454.4 L1104.89 1485.02 L1112.53 1485.02 L1112.53 1488.96 L1092.62 1488.96 L1092.62 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1131.97 1457.48 Q1128.36 1457.48 1126.53 1461.04 Q1124.73 1464.58 1124.73 1471.71 Q1124.73 1478.82 1126.53 1482.38 Q1128.36 1485.92 1131.97 1485.92 Q1135.6 1485.92 1137.41 1482.38 Q1139.24 1478.82 1139.24 1471.71 Q1139.24 1464.58 1137.41 1461.04 Q1135.6 1457.48 1131.97 1457.48 M1131.97 1453.77 Q1137.78 1453.77 1140.84 1458.38 Q1143.91 1462.96 1143.91 1471.71 Q1143.91 1480.44 1140.84 1485.04 Q1137.78 1489.63 1131.97 1489.63 Q1126.16 1489.63 1123.08 1485.04 Q1120.03 1480.44 1120.03 1471.71 Q1120.03 1462.96 1123.08 1458.38 Q1126.16 1453.77 1131.97 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1148.49 1458.35 L1161.74 1458.35 L1161.74 1461.55 L1143.91 1461.55 L1143.91 1458.35 Q1146.08 1456.11 1149.8 1452.35 Q1153.54 1448.57 1154.5 1447.48 Q1156.33 1445.43 1157.04 1444.02 Q1157.78 1442.59 1157.78 1441.22 Q1157.78 1438.98 1156.2 1437.57 Q1154.64 1436.16 1152.12 1436.16 Q1150.33 1436.16 1148.33 1436.78 Q1146.36 1437.4 1144.1 1438.66 L1144.1 1434.82 Q1146.4 1433.9 1148.39 1433.43 Q1150.38 1432.96 1152.04 1432.96 Q1156.4 1432.96 1159 1435.14 Q1161.59 1437.32 1161.59 1440.97 Q1161.59 1442.7 1160.94 1444.26 Q1160.3 1445.8 1158.58 1447.91 Q1158.11 1448.46 1155.59 1451.07 Q1153.07 1453.67 1148.49 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M2163.34 1485.02 L2170.98 1485.02 L2170.98 1458.66 L2162.67 1460.32 L2162.67 1456.06 L2170.93 1454.4 L2175.61 1454.4 L2175.61 1485.02 L2183.25 1485.02 L2183.25 1488.96 L2163.34 1488.96 L2163.34 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M2202.69 1457.48 Q2199.08 1457.48 2197.25 1461.04 Q2195.45 1464.58 2195.45 1471.71 Q2195.45 1478.82 2197.25 1482.38 Q2199.08 1485.92 2202.69 1485.92 Q2206.33 1485.92 2208.13 1482.38 Q2209.96 1478.82 2209.96 1471.71 Q2209.96 1464.58 2208.13 1461.04 Q2206.33 1457.48 2202.69 1457.48 M2202.69 1453.77 Q2208.5 1453.77 2211.56 1458.38 Q2214.64 1462.96 2214.64 1471.71 Q2214.64 1480.44 2211.56 1485.04 Q2208.5 1489.63 2202.69 1489.63 Q2196.88 1489.63 2193.8 1485.04 Q2190.75 1480.44 2190.75 1471.71 Q2190.75 1462.96 2193.8 1458.38 Q2196.88 1453.77 2202.69 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M2227.31 1436.78 L2217.72 1451.77 L2227.31 1451.77 L2227.31 1436.78 M2226.32 1433.47 L2231.09 1433.47 L2231.09 1451.77 L2235.1 1451.77 L2235.1 1454.93 L2231.09 1454.93 L2231.09 1461.55 L2227.31 1461.55 L2227.31 1454.93 L2214.64 1454.93 L2214.64 1451.26 L2226.32 1433.47 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1154.79 1545.38 Q1156.98 1541.43 1160.04 1539.56 Q1163.09 1537.68 1167.23 1537.68 Q1172.8 1537.68 1175.82 1541.59 Q1178.85 1545.48 1178.85 1552.67 L1178.85 1574.19 L1172.96 1574.19 L1172.96 1552.86 Q1172.96 1547.74 1171.15 1545.25 Q1169.33 1542.77 1165.61 1542.77 Q1161.06 1542.77 1158.41 1545.79 Q1155.77 1548.82 1155.77 1554.04 L1155.77 1574.19 L1149.88 1574.19 L1149.88 1552.86 Q1149.88 1547.7 1148.07 1545.25 Q1146.26 1542.77 1142.47 1542.77 Q1137.98 1542.77 1135.34 1545.83 Q1132.7 1548.85 1132.7 1554.04 L1132.7 1574.19 L1126.81 1574.19 L1126.81 1538.54 L1132.7 1538.54 L1132.7 1544.08 Q1134.7 1540.8 1137.5 1539.24 Q1140.3 1537.68 1144.16 1537.68 Q1148.04 1537.68 1150.74 1539.65 Q1153.48 1541.62 1154.79 1545.38 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1206.73 1556.27 Q1199.63 1556.27 1196.9 1557.89 Q1194.16 1559.51 1194.16 1563.43 Q1194.16 1566.55 1196.2 1568.39 Q1198.26 1570.21 1201.8 1570.21 Q1206.67 1570.21 1209.59 1566.77 Q1212.55 1563.3 1212.55 1557.57 L1212.55 1556.27 L1206.73 1556.27 M1218.41 1553.85 L1218.41 1574.19 L1212.55 1574.19 L1212.55 1568.77 Q1210.55 1572.02 1207.56 1573.58 Q1204.57 1575.11 1200.24 1575.11 Q1194.76 1575.11 1191.52 1572.05 Q1188.3 1568.97 1188.3 1563.81 Q1188.3 1557.79 1192.31 1554.74 Q1196.35 1551.68 1204.34 1551.68 L1212.55 1551.68 L1212.55 1551.11 Q1212.55 1547.07 1209.88 1544.87 Q1207.24 1542.64 1202.43 1542.64 Q1199.38 1542.64 1196.48 1543.38 Q1193.59 1544.11 1190.91 1545.57 L1190.91 1540.16 Q1194.13 1538.92 1197.15 1538.31 Q1200.17 1537.68 1203.04 1537.68 Q1210.77 1537.68 1214.59 1541.69 Q1218.41 1545.7 1218.41 1553.85 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1236.27 1528.42 L1236.27 1538.54 L1248.33 1538.54 L1248.33 1543.09 L1236.27 1543.09 L1236.27 1562.44 Q1236.27 1566.8 1237.44 1568.04 Q1238.65 1569.28 1242.31 1569.28 L1248.33 1569.28 L1248.33 1574.19 L1242.31 1574.19 Q1235.54 1574.19 1232.96 1571.67 Q1230.38 1569.12 1230.38 1562.44 L1230.38 1543.09 L1226.08 1543.09 L1226.08 1538.54 L1230.38 1538.54 L1230.38 1528.42 L1236.27 1528.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1276.69 1544.01 Q1275.7 1543.44 1274.53 1543.18 Q1273.38 1542.9 1271.98 1542.9 Q1267.01 1542.9 1264.34 1546.14 Q1261.7 1549.36 1261.7 1555.41 L1261.7 1574.19 L1255.81 1574.19 L1255.81 1538.54 L1261.7 1538.54 L1261.7 1544.08 Q1263.54 1540.83 1266.5 1539.27 Q1269.46 1537.68 1273.7 1537.68 Q1274.3 1537.68 1275.03 1537.77 Q1275.77 1537.84 1276.66 1538 L1276.69 1544.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1282.83 1538.54 L1288.69 1538.54 L1288.69 1574.19 L1282.83 1574.19 L1282.83 1538.54 M1282.83 1524.66 L1288.69 1524.66 L1288.69 1532.08 L1282.83 1532.08 L1282.83 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1330.58 1538.54 L1317.68 1555.88 L1331.24 1574.19 L1324.34 1574.19 L1313.96 1560.18 L1303.58 1574.19 L1296.68 1574.19 L1310.52 1555.53 L1297.86 1538.54 L1304.76 1538.54 L1314.22 1551.24 L1323.67 1538.54 L1330.58 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1382.96 1539.59 L1382.96 1545.13 Q1380.48 1543.85 1377.81 1543.22 Q1375.14 1542.58 1372.27 1542.58 Q1367.91 1542.58 1365.71 1543.92 Q1363.55 1545.25 1363.55 1547.93 Q1363.55 1549.96 1365.11 1551.14 Q1366.67 1552.29 1371.38 1553.34 L1373.38 1553.78 Q1379.62 1555.12 1382.23 1557.57 Q1384.87 1559.99 1384.87 1564.35 Q1384.87 1569.32 1380.93 1572.21 Q1377.01 1575.11 1370.14 1575.11 Q1367.27 1575.11 1364.15 1574.54 Q1361.07 1573.99 1357.63 1572.88 L1357.63 1566.83 Q1360.88 1568.52 1364.03 1569.38 Q1367.18 1570.21 1370.27 1570.21 Q1374.4 1570.21 1376.63 1568.81 Q1378.86 1567.37 1378.86 1564.8 Q1378.86 1562.41 1377.24 1561.14 Q1375.64 1559.86 1370.2 1558.68 L1368.16 1558.21 Q1362.72 1557.06 1360.3 1554.71 Q1357.88 1552.32 1357.88 1548.18 Q1357.88 1543.15 1361.45 1540.42 Q1365.01 1537.68 1371.57 1537.68 Q1374.82 1537.68 1377.68 1538.16 Q1380.55 1538.63 1382.96 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1394.2 1538.54 L1400.06 1538.54 L1400.06 1574.19 L1394.2 1574.19 L1394.2 1538.54 M1394.2 1524.66 L1400.06 1524.66 L1400.06 1532.08 L1394.2 1532.08 L1394.2 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1409.76 1538.54 L1437.58 1538.54 L1437.58 1543.88 L1415.56 1569.51 L1437.58 1569.51 L1437.58 1574.19 L1408.97 1574.19 L1408.97 1568.84 L1430.99 1543.22 L1409.76 1543.22 L1409.76 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1477.02 1554.9 L1477.02 1557.76 L1450.09 1557.76 Q1450.47 1563.81 1453.72 1566.99 Q1457 1570.14 1462.82 1570.14 Q1466.2 1570.14 1469.35 1569.32 Q1472.53 1568.49 1475.65 1566.83 L1475.65 1572.37 Q1472.5 1573.71 1469.19 1574.41 Q1465.88 1575.11 1462.47 1575.11 Q1453.94 1575.11 1448.95 1570.14 Q1443.98 1565.18 1443.98 1556.71 Q1443.98 1547.96 1448.69 1542.83 Q1453.43 1537.68 1461.45 1537.68 Q1468.65 1537.68 1472.82 1542.33 Q1477.02 1546.94 1477.02 1554.9 M1471.16 1553.18 Q1471.1 1548.37 1468.46 1545.51 Q1465.85 1542.64 1461.52 1542.64 Q1456.62 1542.64 1453.66 1545.41 Q1450.73 1548.18 1450.28 1553.21 L1471.16 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 251.071,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1168.47 269.969,1168.47 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,759.911 269.969,759.911 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,351.353 269.969,351.353 \"/>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M114.931 1188.26 L122.57 1188.26 L122.57 1161.9 L114.26 1163.56 L114.26 1159.3 L122.524 1157.64 L127.2 1157.64 L127.2 1188.26 L134.839 1188.26 L134.839 1192.2 L114.931 1192.2 L114.931 1188.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M154.283 1160.71 Q150.672 1160.71 148.843 1164.28 Q147.038 1167.82 147.038 1174.95 Q147.038 1182.06 148.843 1185.62 Q150.672 1189.16 154.283 1189.16 Q157.917 1189.16 159.723 1185.62 Q161.552 1182.06 161.552 1174.95 Q161.552 1167.82 159.723 1164.28 Q157.917 1160.71 154.283 1160.71 M154.283 1157.01 Q160.093 1157.01 163.149 1161.62 Q166.227 1166.2 166.227 1174.95 Q166.227 1183.68 163.149 1188.28 Q160.093 1192.87 154.283 1192.87 Q148.473 1192.87 145.394 1188.28 Q142.339 1183.68 142.339 1174.95 Q142.339 1166.2 145.394 1161.62 Q148.473 1157.01 154.283 1157.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M166.227 1151.11 L190.339 1151.11 L190.339 1154.31 L166.227 1154.31 L166.227 1151.11 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M201.812 1161.59 L215.071 1161.59 L215.071 1164.79 L197.241 1164.79 L197.241 1161.59 Q199.404 1159.35 203.128 1155.59 Q206.871 1151.81 207.83 1150.72 Q209.655 1148.67 210.369 1147.26 Q211.103 1145.83 211.103 1144.45 Q211.103 1142.22 209.523 1140.81 Q207.962 1139.4 205.442 1139.4 Q203.655 1139.4 201.661 1140.02 Q199.686 1140.64 197.43 1141.9 L197.43 1138.06 Q199.724 1137.14 201.718 1136.67 Q203.711 1136.2 205.366 1136.2 Q209.73 1136.2 212.325 1138.38 Q214.921 1140.56 214.921 1144.21 Q214.921 1145.94 214.263 1147.5 Q213.623 1149.04 211.912 1151.15 Q211.441 1151.7 208.921 1154.31 Q206.401 1156.91 201.812 1161.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M144.366 779.703 L152.004 779.703 L152.004 753.338 L143.694 755.004 L143.694 750.745 L151.958 749.078 L156.634 749.078 L156.634 779.703 L164.273 779.703 L164.273 783.638 L144.366 783.638 L144.366 779.703 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M183.717 752.157 Q180.106 752.157 178.277 755.722 Q176.472 759.264 176.472 766.393 Q176.472 773.5 178.277 777.064 Q180.106 780.606 183.717 780.606 Q187.351 780.606 189.157 777.064 Q190.986 773.5 190.986 766.393 Q190.986 759.264 189.157 755.722 Q187.351 752.157 183.717 752.157 M183.717 748.453 Q189.527 748.453 192.583 753.06 Q195.662 757.643 195.662 766.393 Q195.662 775.12 192.583 779.726 Q189.527 784.31 183.717 784.31 Q177.907 784.31 174.828 779.726 Q171.773 775.12 171.773 766.393 Q171.773 757.643 174.828 753.06 Q177.907 748.453 183.717 748.453 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M205.366 730.65 Q202.432 730.65 200.947 733.546 Q199.48 736.423 199.48 742.216 Q199.48 747.99 200.947 750.887 Q202.432 753.764 205.366 753.764 Q208.319 753.764 209.786 750.887 Q211.272 747.99 211.272 742.216 Q211.272 736.423 209.786 733.546 Q208.319 730.65 205.366 730.65 M205.366 727.64 Q210.087 727.64 212.57 731.383 Q215.071 735.107 215.071 742.216 Q215.071 749.307 212.57 753.05 Q210.087 756.773 205.366 756.773 Q200.646 756.773 198.144 753.05 Q195.662 749.307 195.662 742.216 Q195.662 735.107 198.144 731.383 Q200.646 727.64 205.366 727.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M145.945 371.146 L153.584 371.146 L153.584 344.78 L145.274 346.447 L145.274 342.187 L153.538 340.521 L158.214 340.521 L158.214 371.146 L165.853 371.146 L165.853 375.081 L145.945 375.081 L145.945 371.146 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M185.297 343.599 Q181.686 343.599 179.857 347.164 Q178.052 350.706 178.052 357.836 Q178.052 364.942 179.857 368.507 Q181.686 372.048 185.297 372.048 Q188.931 372.048 190.737 368.507 Q192.566 364.942 192.566 357.836 Q192.566 350.706 190.737 347.164 Q188.931 343.599 185.297 343.599 M185.297 339.896 Q191.107 339.896 194.163 344.502 Q197.241 349.086 197.241 357.836 Q197.241 366.562 194.163 371.169 Q191.107 375.752 185.297 375.752 Q179.487 375.752 176.408 371.169 Q173.353 366.562 173.353 357.836 Q173.353 349.086 176.408 344.502 Q179.487 339.896 185.297 339.896 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M201.812 344.473 L215.071 344.473 L215.071 347.67 L197.241 347.67 L197.241 344.473 Q199.404 342.235 203.128 338.473 Q206.871 334.693 207.83 333.602 Q209.655 331.552 210.369 330.142 Q211.103 328.712 211.103 327.339 Q211.103 325.101 209.523 323.69 Q207.962 322.28 205.442 322.28 Q203.655 322.28 201.661 322.901 Q199.686 323.521 197.43 324.781 L197.43 320.945 Q199.724 320.023 201.718 319.553 Q203.711 319.083 205.366 319.083 Q209.73 319.083 212.325 321.264 Q214.921 323.446 214.921 327.095 Q214.921 328.825 214.263 330.386 Q213.623 331.928 211.912 334.035 Q211.441 334.58 208.921 337.194 Q206.401 339.79 201.812 344.473 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M57.2247 909.252 L44.4614 909.252 L44.4614 919.755 L39.1779 919.755 L39.1779 902.886 L59.58 902.886 Q62.2218 906.61 63.5904 911.098 Q64.9272 915.586 64.9272 920.678 Q64.9272 931.818 58.4342 938.12 Q51.9093 944.39 40.2919 944.39 Q28.6427 944.39 22.1496 938.12 Q15.6248 931.818 15.6248 920.678 Q15.6248 916.031 16.7706 911.862 Q17.9164 907.66 20.1444 904.127 L26.9876 904.127 Q23.9639 907.692 22.4361 911.702 Q20.9083 915.713 20.9083 920.137 Q20.9083 928.858 25.7781 933.25 Q30.6479 937.611 40.2919 937.611 Q49.9041 937.611 54.7739 933.25 Q59.6436 928.858 59.6436 920.137 Q59.6436 916.731 59.0707 914.058 Q58.466 911.384 57.2247 909.252 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M21.7677 884.712 L39.6235 884.712 L39.6235 876.627 Q39.6235 872.14 37.3 869.689 Q34.9765 867.238 30.6797 867.238 Q26.4147 867.238 24.0912 869.689 Q21.7677 872.14 21.7677 876.627 L21.7677 884.712 M16.4842 891.141 L16.4842 876.627 Q16.4842 868.638 20.1126 864.564 Q23.7092 860.459 30.6797 860.459 Q37.7138 860.459 41.3104 864.564 Q44.907 868.638 44.907 876.627 L44.907 884.712 L64.0042 884.712 L64.0042 891.141 L16.4842 891.141 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M16.4842 852.565 L16.4842 846.104 L45.3526 846.104 Q52.9915 846.104 56.3653 843.335 Q59.7073 840.566 59.7073 834.359 Q59.7073 828.184 56.3653 825.415 Q52.9915 822.646 45.3526 822.646 L16.4842 822.646 L16.4842 816.185 L46.1484 816.185 Q55.4423 816.185 60.1847 820.8 Q64.9272 825.384 64.9272 834.359 Q64.9272 843.367 60.1847 847.982 Q55.4423 852.565 46.1484 852.565 L16.4842 852.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M29.4065 760.931 L34.9447 760.931 Q33.6716 763.413 33.035 766.087 Q32.3984 768.761 32.3984 771.625 Q32.3984 775.986 33.7352 778.182 Q35.072 780.346 37.7456 780.346 Q39.7826 780.346 40.9603 778.787 Q42.1061 777.227 43.1565 772.516 L43.6021 770.511 Q44.9389 764.273 47.3897 761.663 Q49.8086 759.021 54.1691 759.021 Q59.1344 759.021 62.0308 762.968 Q64.9272 766.883 64.9272 773.758 Q64.9272 776.622 64.3543 779.741 Q63.8132 782.829 62.6992 786.266 L56.6518 786.266 Q58.3387 783.02 59.198 779.869 Q60.0256 776.718 60.0256 773.63 Q60.0256 769.493 58.6251 767.265 Q57.1929 765.037 54.6147 765.037 Q52.2276 765.037 50.9545 766.66 Q49.6813 768.251 48.5037 773.694 L48.0262 775.731 Q46.8804 781.174 44.5251 783.593 Q42.138 786.012 38.0002 786.012 Q32.9713 786.012 30.2341 782.447 Q27.4968 778.882 27.4968 772.325 Q27.4968 769.079 27.9743 766.214 Q28.4517 763.35 29.4065 760.931 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M58.657 744.03 L77.5631 744.03 L77.5631 749.918 L28.3562 749.918 L28.3562 744.03 L33.7671 744.03 Q30.5842 742.184 29.0564 739.383 Q27.4968 736.55 27.4968 732.635 Q27.4968 726.142 32.6531 722.1 Q37.8093 718.026 46.212 718.026 Q54.6147 718.026 59.771 722.1 Q64.9272 726.142 64.9272 732.635 Q64.9272 736.55 63.3994 739.383 Q61.8398 742.184 58.657 744.03 M46.212 724.105 Q39.7508 724.105 36.0905 726.779 Q32.3984 729.42 32.3984 734.067 Q32.3984 738.714 36.0905 741.388 Q39.7508 744.03 46.212 744.03 Q52.6732 744.03 56.3653 741.388 Q60.0256 738.714 60.0256 734.067 Q60.0256 729.42 56.3653 726.779 Q52.6732 724.105 46.212 724.105 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M44.7161 677.826 L47.5806 677.826 L47.5806 704.753 Q53.6281 704.371 56.8109 701.125 Q59.9619 697.847 59.9619 692.022 Q59.9619 688.648 59.1344 685.497 Q58.3069 682.314 56.6518 679.195 L62.1899 679.195 Q63.5267 682.346 64.227 685.656 Q64.9272 688.966 64.9272 692.372 Q64.9272 700.902 59.9619 705.899 Q54.9967 710.864 46.5303 710.864 Q37.7774 710.864 32.6531 706.154 Q27.4968 701.411 27.4968 693.391 Q27.4968 686.197 32.1438 682.028 Q36.7589 677.826 44.7161 677.826 M42.9973 683.683 Q38.1912 683.747 35.3266 686.388 Q32.4621 688.998 32.4621 693.327 Q32.4621 698.229 35.2312 701.189 Q38.0002 704.117 43.0292 704.562 L42.9973 683.683 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M44.7161 637.722 L47.5806 637.722 L47.5806 664.649 Q53.6281 664.267 56.8109 661.021 Q59.9619 657.743 59.9619 651.918 Q59.9619 648.544 59.1344 645.393 Q58.3069 642.21 56.6518 639.091 L62.1899 639.091 Q63.5267 642.242 64.227 645.552 Q64.9272 648.862 64.9272 652.268 Q64.9272 660.798 59.9619 665.795 Q54.9967 670.76 46.5303 670.76 Q37.7774 670.76 32.6531 666.05 Q27.4968 661.307 27.4968 653.287 Q27.4968 646.093 32.1438 641.924 Q36.7589 637.722 44.7161 637.722 M42.9973 643.579 Q38.1912 643.643 35.3266 646.284 Q32.4621 648.894 32.4621 653.223 Q32.4621 658.125 35.2312 661.085 Q38.0002 664.013 43.0292 664.458 L42.9973 643.579 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M33.7671 604.653 L14.479 604.653 L14.479 598.796 L64.0042 598.796 L64.0042 604.653 L58.657 604.653 Q61.8398 606.499 63.3994 609.331 Q64.9272 612.132 64.9272 616.079 Q64.9272 622.54 59.771 626.614 Q54.6147 630.657 46.212 630.657 Q37.8093 630.657 32.6531 626.614 Q27.4968 622.54 27.4968 616.079 Q27.4968 612.132 29.0564 609.331 Q30.5842 606.499 33.7671 604.653 M46.212 624.609 Q52.6732 624.609 56.3653 621.967 Q60.0256 619.294 60.0256 614.647 Q60.0256 610 56.3653 607.326 Q52.6732 604.653 46.212 604.653 Q39.7508 604.653 36.0905 607.326 Q32.3984 610 32.3984 614.647 Q32.3984 619.294 36.0905 621.967 Q39.7508 624.609 46.212 624.609 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M49.9359 587.338 L28.3562 587.338 L28.3562 581.481 L49.7131 581.481 Q54.7739 581.481 57.3202 579.508 Q59.8346 577.535 59.8346 573.588 Q59.8346 568.846 56.8109 566.108 Q53.7872 563.339 48.5673 563.339 L28.3562 563.339 L28.3562 557.483 L64.0042 557.483 L64.0042 563.339 L58.5296 563.339 Q61.7762 565.472 63.3676 568.304 Q64.9272 571.105 64.9272 574.829 Q64.9272 580.972 61.1078 584.155 Q57.2883 587.338 49.9359 587.338 M27.4968 572.601 L27.4968 572.601 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M58.657 539.754 L77.5631 539.754 L77.5631 545.643 L28.3562 545.643 L28.3562 539.754 L33.7671 539.754 Q30.5842 537.908 29.0564 535.107 Q27.4968 532.275 27.4968 528.36 Q27.4968 521.867 32.6531 517.824 Q37.8093 513.75 46.212 513.75 Q54.6147 513.75 59.771 517.824 Q64.9272 521.867 64.9272 528.36 Q64.9272 532.275 63.3994 535.107 Q61.8398 537.908 58.657 539.754 M46.212 519.83 Q39.7508 519.83 36.0905 522.503 Q32.3984 525.145 32.3984 529.792 Q32.3984 534.439 36.0905 537.112 Q39.7508 539.754 46.212 539.754 Q52.6732 539.754 56.3653 537.112 Q60.0256 534.439 60.0256 529.792 Q60.0256 525.145 56.3653 522.503 Q52.6732 519.83 46.212 519.83 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip012)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"310.553,1372.3 429.468,1270.12 590.826,1222.53 1126.85,744.316 1501.51,557.133 1662.87,230.934 2037.53,158.811 2198.89,107.941 2293.27,85.838 \"/>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M2020.1 196.379 L2282.7 196.379 L2282.7 92.6992 L2020.1 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2020.1,196.379 2282.7,196.379 2282.7,92.6992 2020.1,92.6992 2020.1,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip010)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2043.45,144.539 2183.57,144.539 \"/>\n",
       "<path clip-path=\"url(#clip010)\" d=\"M2220.76 164.227 Q2218.95 168.856 2217.24 170.268 Q2215.53 171.68 2212.66 171.68 L2209.26 171.68 L2209.26 168.115 L2211.76 168.115 Q2213.51 168.115 2214.49 167.282 Q2215.46 166.449 2216.64 163.347 L2217.4 161.403 L2206.92 135.893 L2211.43 135.893 L2219.53 156.171 L2227.63 135.893 L2232.15 135.893 L2220.76 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M2239.44 157.884 L2247.08 157.884 L2247.08 131.518 L2238.77 133.185 L2238.77 128.926 L2247.03 127.259 L2251.71 127.259 L2251.71 157.884 L2259.35 157.884 L2259.35 161.819 L2239.44 161.819 L2239.44 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip060\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip061\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M251.071 1410.9 L2352.76 1410.9 L2352.76 47.2441 L251.071 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip062\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2103\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1126.85,1410.9 1126.85,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2198.89,1410.9 2198.89,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,1168.47 2352.76,1168.47 \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,759.911 2352.76,759.911 \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"251.071,351.353 2352.76,351.353 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1126.85,1410.9 1126.85,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2198.89,1410.9 2198.89,1392 \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M1092.62 1485.02 L1100.26 1485.02 L1100.26 1458.66 L1091.95 1460.32 L1091.95 1456.06 L1100.21 1454.4 L1104.89 1454.4 L1104.89 1485.02 L1112.53 1485.02 L1112.53 1488.96 L1092.62 1488.96 L1092.62 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1131.97 1457.48 Q1128.36 1457.48 1126.53 1461.04 Q1124.73 1464.58 1124.73 1471.71 Q1124.73 1478.82 1126.53 1482.38 Q1128.36 1485.92 1131.97 1485.92 Q1135.6 1485.92 1137.41 1482.38 Q1139.24 1478.82 1139.24 1471.71 Q1139.24 1464.58 1137.41 1461.04 Q1135.6 1457.48 1131.97 1457.48 M1131.97 1453.77 Q1137.78 1453.77 1140.84 1458.38 Q1143.91 1462.96 1143.91 1471.71 Q1143.91 1480.44 1140.84 1485.04 Q1137.78 1489.63 1131.97 1489.63 Q1126.16 1489.63 1123.08 1485.04 Q1120.03 1480.44 1120.03 1471.71 Q1120.03 1462.96 1123.08 1458.38 Q1126.16 1453.77 1131.97 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1148.49 1458.35 L1161.74 1458.35 L1161.74 1461.55 L1143.91 1461.55 L1143.91 1458.35 Q1146.08 1456.11 1149.8 1452.35 Q1153.54 1448.57 1154.5 1447.48 Q1156.33 1445.43 1157.04 1444.02 Q1157.78 1442.59 1157.78 1441.22 Q1157.78 1438.98 1156.2 1437.57 Q1154.64 1436.16 1152.12 1436.16 Q1150.33 1436.16 1148.33 1436.78 Q1146.36 1437.4 1144.1 1438.66 L1144.1 1434.82 Q1146.4 1433.9 1148.39 1433.43 Q1150.38 1432.96 1152.04 1432.96 Q1156.4 1432.96 1159 1435.14 Q1161.59 1437.32 1161.59 1440.97 Q1161.59 1442.7 1160.94 1444.26 Q1160.3 1445.8 1158.58 1447.91 Q1158.11 1448.46 1155.59 1451.07 Q1153.07 1453.67 1148.49 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2163.34 1485.02 L2170.98 1485.02 L2170.98 1458.66 L2162.67 1460.32 L2162.67 1456.06 L2170.93 1454.4 L2175.61 1454.4 L2175.61 1485.02 L2183.25 1485.02 L2183.25 1488.96 L2163.34 1488.96 L2163.34 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2202.69 1457.48 Q2199.08 1457.48 2197.25 1461.04 Q2195.45 1464.58 2195.45 1471.71 Q2195.45 1478.82 2197.25 1482.38 Q2199.08 1485.92 2202.69 1485.92 Q2206.33 1485.92 2208.13 1482.38 Q2209.96 1478.82 2209.96 1471.71 Q2209.96 1464.58 2208.13 1461.04 Q2206.33 1457.48 2202.69 1457.48 M2202.69 1453.77 Q2208.5 1453.77 2211.56 1458.38 Q2214.64 1462.96 2214.64 1471.71 Q2214.64 1480.44 2211.56 1485.04 Q2208.5 1489.63 2202.69 1489.63 Q2196.88 1489.63 2193.8 1485.04 Q2190.75 1480.44 2190.75 1471.71 Q2190.75 1462.96 2193.8 1458.38 Q2196.88 1453.77 2202.69 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2227.31 1436.78 L2217.72 1451.77 L2227.31 1451.77 L2227.31 1436.78 M2226.32 1433.47 L2231.09 1433.47 L2231.09 1451.77 L2235.1 1451.77 L2235.1 1454.93 L2231.09 1454.93 L2231.09 1461.55 L2227.31 1461.55 L2227.31 1454.93 L2214.64 1454.93 L2214.64 1451.26 L2226.32 1433.47 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1154.79 1545.38 Q1156.98 1541.43 1160.04 1539.56 Q1163.09 1537.68 1167.23 1537.68 Q1172.8 1537.68 1175.82 1541.59 Q1178.85 1545.48 1178.85 1552.67 L1178.85 1574.19 L1172.96 1574.19 L1172.96 1552.86 Q1172.96 1547.74 1171.15 1545.25 Q1169.33 1542.77 1165.61 1542.77 Q1161.06 1542.77 1158.41 1545.79 Q1155.77 1548.82 1155.77 1554.04 L1155.77 1574.19 L1149.88 1574.19 L1149.88 1552.86 Q1149.88 1547.7 1148.07 1545.25 Q1146.26 1542.77 1142.47 1542.77 Q1137.98 1542.77 1135.34 1545.83 Q1132.7 1548.85 1132.7 1554.04 L1132.7 1574.19 L1126.81 1574.19 L1126.81 1538.54 L1132.7 1538.54 L1132.7 1544.08 Q1134.7 1540.8 1137.5 1539.24 Q1140.3 1537.68 1144.16 1537.68 Q1148.04 1537.68 1150.74 1539.65 Q1153.48 1541.62 1154.79 1545.38 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1206.73 1556.27 Q1199.63 1556.27 1196.9 1557.89 Q1194.16 1559.51 1194.16 1563.43 Q1194.16 1566.55 1196.2 1568.39 Q1198.26 1570.21 1201.8 1570.21 Q1206.67 1570.21 1209.59 1566.77 Q1212.55 1563.3 1212.55 1557.57 L1212.55 1556.27 L1206.73 1556.27 M1218.41 1553.85 L1218.41 1574.19 L1212.55 1574.19 L1212.55 1568.77 Q1210.55 1572.02 1207.56 1573.58 Q1204.57 1575.11 1200.24 1575.11 Q1194.76 1575.11 1191.52 1572.05 Q1188.3 1568.97 1188.3 1563.81 Q1188.3 1557.79 1192.31 1554.74 Q1196.35 1551.68 1204.34 1551.68 L1212.55 1551.68 L1212.55 1551.11 Q1212.55 1547.07 1209.88 1544.87 Q1207.24 1542.64 1202.43 1542.64 Q1199.38 1542.64 1196.48 1543.38 Q1193.59 1544.11 1190.91 1545.57 L1190.91 1540.16 Q1194.13 1538.92 1197.15 1538.31 Q1200.17 1537.68 1203.04 1537.68 Q1210.77 1537.68 1214.59 1541.69 Q1218.41 1545.7 1218.41 1553.85 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1236.27 1528.42 L1236.27 1538.54 L1248.33 1538.54 L1248.33 1543.09 L1236.27 1543.09 L1236.27 1562.44 Q1236.27 1566.8 1237.44 1568.04 Q1238.65 1569.28 1242.31 1569.28 L1248.33 1569.28 L1248.33 1574.19 L1242.31 1574.19 Q1235.54 1574.19 1232.96 1571.67 Q1230.38 1569.12 1230.38 1562.44 L1230.38 1543.09 L1226.08 1543.09 L1226.08 1538.54 L1230.38 1538.54 L1230.38 1528.42 L1236.27 1528.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1276.69 1544.01 Q1275.7 1543.44 1274.53 1543.18 Q1273.38 1542.9 1271.98 1542.9 Q1267.01 1542.9 1264.34 1546.14 Q1261.7 1549.36 1261.7 1555.41 L1261.7 1574.19 L1255.81 1574.19 L1255.81 1538.54 L1261.7 1538.54 L1261.7 1544.08 Q1263.54 1540.83 1266.5 1539.27 Q1269.46 1537.68 1273.7 1537.68 Q1274.3 1537.68 1275.03 1537.77 Q1275.77 1537.84 1276.66 1538 L1276.69 1544.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1282.83 1538.54 L1288.69 1538.54 L1288.69 1574.19 L1282.83 1574.19 L1282.83 1538.54 M1282.83 1524.66 L1288.69 1524.66 L1288.69 1532.08 L1282.83 1532.08 L1282.83 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1330.58 1538.54 L1317.68 1555.88 L1331.24 1574.19 L1324.34 1574.19 L1313.96 1560.18 L1303.58 1574.19 L1296.68 1574.19 L1310.52 1555.53 L1297.86 1538.54 L1304.76 1538.54 L1314.22 1551.24 L1323.67 1538.54 L1330.58 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1382.96 1539.59 L1382.96 1545.13 Q1380.48 1543.85 1377.81 1543.22 Q1375.14 1542.58 1372.27 1542.58 Q1367.91 1542.58 1365.71 1543.92 Q1363.55 1545.25 1363.55 1547.93 Q1363.55 1549.96 1365.11 1551.14 Q1366.67 1552.29 1371.38 1553.34 L1373.38 1553.78 Q1379.62 1555.12 1382.23 1557.57 Q1384.87 1559.99 1384.87 1564.35 Q1384.87 1569.32 1380.93 1572.21 Q1377.01 1575.11 1370.14 1575.11 Q1367.27 1575.11 1364.15 1574.54 Q1361.07 1573.99 1357.63 1572.88 L1357.63 1566.83 Q1360.88 1568.52 1364.03 1569.38 Q1367.18 1570.21 1370.27 1570.21 Q1374.4 1570.21 1376.63 1568.81 Q1378.86 1567.37 1378.86 1564.8 Q1378.86 1562.41 1377.24 1561.14 Q1375.64 1559.86 1370.2 1558.68 L1368.16 1558.21 Q1362.72 1557.06 1360.3 1554.71 Q1357.88 1552.32 1357.88 1548.18 Q1357.88 1543.15 1361.45 1540.42 Q1365.01 1537.68 1371.57 1537.68 Q1374.82 1537.68 1377.68 1538.16 Q1380.55 1538.63 1382.96 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1394.2 1538.54 L1400.06 1538.54 L1400.06 1574.19 L1394.2 1574.19 L1394.2 1538.54 M1394.2 1524.66 L1400.06 1524.66 L1400.06 1532.08 L1394.2 1532.08 L1394.2 1524.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1409.76 1538.54 L1437.58 1538.54 L1437.58 1543.88 L1415.56 1569.51 L1437.58 1569.51 L1437.58 1574.19 L1408.97 1574.19 L1408.97 1568.84 L1430.99 1543.22 L1409.76 1543.22 L1409.76 1538.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1477.02 1554.9 L1477.02 1557.76 L1450.09 1557.76 Q1450.47 1563.81 1453.72 1566.99 Q1457 1570.14 1462.82 1570.14 Q1466.2 1570.14 1469.35 1569.32 Q1472.53 1568.49 1475.65 1566.83 L1475.65 1572.37 Q1472.5 1573.71 1469.19 1574.41 Q1465.88 1575.11 1462.47 1575.11 Q1453.94 1575.11 1448.95 1570.14 Q1443.98 1565.18 1443.98 1556.71 Q1443.98 1547.96 1448.69 1542.83 Q1453.43 1537.68 1461.45 1537.68 Q1468.65 1537.68 1472.82 1542.33 Q1477.02 1546.94 1477.02 1554.9 M1471.16 1553.18 Q1471.1 1548.37 1468.46 1545.51 Q1465.85 1542.64 1461.52 1542.64 Q1456.62 1542.64 1453.66 1545.41 Q1450.73 1548.18 1450.28 1553.21 L1471.16 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1410.9 251.071,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,1168.47 269.969,1168.47 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,759.911 269.969,759.911 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"251.071,351.353 269.969,351.353 \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M114.931 1188.26 L122.57 1188.26 L122.57 1161.9 L114.26 1163.56 L114.26 1159.3 L122.524 1157.64 L127.2 1157.64 L127.2 1188.26 L134.839 1188.26 L134.839 1192.2 L114.931 1192.2 L114.931 1188.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M154.283 1160.71 Q150.672 1160.71 148.843 1164.28 Q147.038 1167.82 147.038 1174.95 Q147.038 1182.06 148.843 1185.62 Q150.672 1189.16 154.283 1189.16 Q157.917 1189.16 159.723 1185.62 Q161.552 1182.06 161.552 1174.95 Q161.552 1167.82 159.723 1164.28 Q157.917 1160.71 154.283 1160.71 M154.283 1157.01 Q160.093 1157.01 163.149 1161.62 Q166.227 1166.2 166.227 1174.95 Q166.227 1183.68 163.149 1188.28 Q160.093 1192.87 154.283 1192.87 Q148.473 1192.87 145.394 1188.28 Q142.339 1183.68 142.339 1174.95 Q142.339 1166.2 145.394 1161.62 Q148.473 1157.01 154.283 1157.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M166.227 1151.11 L190.339 1151.11 L190.339 1154.31 L166.227 1154.31 L166.227 1151.11 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M201.812 1161.59 L215.071 1161.59 L215.071 1164.79 L197.241 1164.79 L197.241 1161.59 Q199.404 1159.35 203.128 1155.59 Q206.871 1151.81 207.83 1150.72 Q209.655 1148.67 210.369 1147.26 Q211.103 1145.83 211.103 1144.45 Q211.103 1142.22 209.523 1140.81 Q207.962 1139.4 205.442 1139.4 Q203.655 1139.4 201.661 1140.02 Q199.686 1140.64 197.43 1141.9 L197.43 1138.06 Q199.724 1137.14 201.718 1136.67 Q203.711 1136.2 205.366 1136.2 Q209.73 1136.2 212.325 1138.38 Q214.921 1140.56 214.921 1144.21 Q214.921 1145.94 214.263 1147.5 Q213.623 1149.04 211.912 1151.15 Q211.441 1151.7 208.921 1154.31 Q206.401 1156.91 201.812 1161.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M144.366 779.703 L152.004 779.703 L152.004 753.338 L143.694 755.004 L143.694 750.745 L151.958 749.078 L156.634 749.078 L156.634 779.703 L164.273 779.703 L164.273 783.638 L144.366 783.638 L144.366 779.703 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M183.717 752.157 Q180.106 752.157 178.277 755.722 Q176.472 759.264 176.472 766.393 Q176.472 773.5 178.277 777.064 Q180.106 780.606 183.717 780.606 Q187.351 780.606 189.157 777.064 Q190.986 773.5 190.986 766.393 Q190.986 759.264 189.157 755.722 Q187.351 752.157 183.717 752.157 M183.717 748.453 Q189.527 748.453 192.583 753.06 Q195.662 757.643 195.662 766.393 Q195.662 775.12 192.583 779.726 Q189.527 784.31 183.717 784.31 Q177.907 784.31 174.828 779.726 Q171.773 775.12 171.773 766.393 Q171.773 757.643 174.828 753.06 Q177.907 748.453 183.717 748.453 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M205.366 730.65 Q202.432 730.65 200.947 733.546 Q199.48 736.423 199.48 742.216 Q199.48 747.99 200.947 750.887 Q202.432 753.764 205.366 753.764 Q208.319 753.764 209.786 750.887 Q211.272 747.99 211.272 742.216 Q211.272 736.423 209.786 733.546 Q208.319 730.65 205.366 730.65 M205.366 727.64 Q210.087 727.64 212.57 731.383 Q215.071 735.107 215.071 742.216 Q215.071 749.307 212.57 753.05 Q210.087 756.773 205.366 756.773 Q200.646 756.773 198.144 753.05 Q195.662 749.307 195.662 742.216 Q195.662 735.107 198.144 731.383 Q200.646 727.64 205.366 727.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M145.945 371.146 L153.584 371.146 L153.584 344.78 L145.274 346.447 L145.274 342.187 L153.538 340.521 L158.214 340.521 L158.214 371.146 L165.853 371.146 L165.853 375.081 L145.945 375.081 L145.945 371.146 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M185.297 343.599 Q181.686 343.599 179.857 347.164 Q178.052 350.706 178.052 357.836 Q178.052 364.942 179.857 368.507 Q181.686 372.048 185.297 372.048 Q188.931 372.048 190.737 368.507 Q192.566 364.942 192.566 357.836 Q192.566 350.706 190.737 347.164 Q188.931 343.599 185.297 343.599 M185.297 339.896 Q191.107 339.896 194.163 344.502 Q197.241 349.086 197.241 357.836 Q197.241 366.562 194.163 371.169 Q191.107 375.752 185.297 375.752 Q179.487 375.752 176.408 371.169 Q173.353 366.562 173.353 357.836 Q173.353 349.086 176.408 344.502 Q179.487 339.896 185.297 339.896 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M201.812 344.473 L215.071 344.473 L215.071 347.67 L197.241 347.67 L197.241 344.473 Q199.404 342.235 203.128 338.473 Q206.871 334.693 207.83 333.602 Q209.655 331.552 210.369 330.142 Q211.103 328.712 211.103 327.339 Q211.103 325.101 209.523 323.69 Q207.962 322.28 205.442 322.28 Q203.655 322.28 201.661 322.901 Q199.686 323.521 197.43 324.781 L197.43 320.945 Q199.724 320.023 201.718 319.553 Q203.711 319.083 205.366 319.083 Q209.73 319.083 212.325 321.264 Q214.921 323.446 214.921 327.095 Q214.921 328.825 214.263 330.386 Q213.623 331.928 211.912 334.035 Q211.441 334.58 208.921 337.194 Q206.401 339.79 201.812 344.473 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M57.2247 909.252 L44.4614 909.252 L44.4614 919.755 L39.1779 919.755 L39.1779 902.886 L59.58 902.886 Q62.2218 906.61 63.5904 911.098 Q64.9272 915.586 64.9272 920.678 Q64.9272 931.818 58.4342 938.12 Q51.9093 944.39 40.2919 944.39 Q28.6427 944.39 22.1496 938.12 Q15.6248 931.818 15.6248 920.678 Q15.6248 916.031 16.7706 911.862 Q17.9164 907.66 20.1444 904.127 L26.9876 904.127 Q23.9639 907.692 22.4361 911.702 Q20.9083 915.713 20.9083 920.137 Q20.9083 928.858 25.7781 933.25 Q30.6479 937.611 40.2919 937.611 Q49.9041 937.611 54.7739 933.25 Q59.6436 928.858 59.6436 920.137 Q59.6436 916.731 59.0707 914.058 Q58.466 911.384 57.2247 909.252 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M21.7677 884.712 L39.6235 884.712 L39.6235 876.627 Q39.6235 872.14 37.3 869.689 Q34.9765 867.238 30.6797 867.238 Q26.4147 867.238 24.0912 869.689 Q21.7677 872.14 21.7677 876.627 L21.7677 884.712 M16.4842 891.141 L16.4842 876.627 Q16.4842 868.638 20.1126 864.564 Q23.7092 860.459 30.6797 860.459 Q37.7138 860.459 41.3104 864.564 Q44.907 868.638 44.907 876.627 L44.907 884.712 L64.0042 884.712 L64.0042 891.141 L16.4842 891.141 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M16.4842 852.565 L16.4842 846.104 L45.3526 846.104 Q52.9915 846.104 56.3653 843.335 Q59.7073 840.566 59.7073 834.359 Q59.7073 828.184 56.3653 825.415 Q52.9915 822.646 45.3526 822.646 L16.4842 822.646 L16.4842 816.185 L46.1484 816.185 Q55.4423 816.185 60.1847 820.8 Q64.9272 825.384 64.9272 834.359 Q64.9272 843.367 60.1847 847.982 Q55.4423 852.565 46.1484 852.565 L16.4842 852.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M29.4065 760.931 L34.9447 760.931 Q33.6716 763.413 33.035 766.087 Q32.3984 768.761 32.3984 771.625 Q32.3984 775.986 33.7352 778.182 Q35.072 780.346 37.7456 780.346 Q39.7826 780.346 40.9603 778.787 Q42.1061 777.227 43.1565 772.516 L43.6021 770.511 Q44.9389 764.273 47.3897 761.663 Q49.8086 759.021 54.1691 759.021 Q59.1344 759.021 62.0308 762.968 Q64.9272 766.883 64.9272 773.758 Q64.9272 776.622 64.3543 779.741 Q63.8132 782.829 62.6992 786.266 L56.6518 786.266 Q58.3387 783.02 59.198 779.869 Q60.0256 776.718 60.0256 773.63 Q60.0256 769.493 58.6251 767.265 Q57.1929 765.037 54.6147 765.037 Q52.2276 765.037 50.9545 766.66 Q49.6813 768.251 48.5037 773.694 L48.0262 775.731 Q46.8804 781.174 44.5251 783.593 Q42.138 786.012 38.0002 786.012 Q32.9713 786.012 30.2341 782.447 Q27.4968 778.882 27.4968 772.325 Q27.4968 769.079 27.9743 766.214 Q28.4517 763.35 29.4065 760.931 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M58.657 744.03 L77.5631 744.03 L77.5631 749.918 L28.3562 749.918 L28.3562 744.03 L33.7671 744.03 Q30.5842 742.184 29.0564 739.383 Q27.4968 736.55 27.4968 732.635 Q27.4968 726.142 32.6531 722.1 Q37.8093 718.026 46.212 718.026 Q54.6147 718.026 59.771 722.1 Q64.9272 726.142 64.9272 732.635 Q64.9272 736.55 63.3994 739.383 Q61.8398 742.184 58.657 744.03 M46.212 724.105 Q39.7508 724.105 36.0905 726.779 Q32.3984 729.42 32.3984 734.067 Q32.3984 738.714 36.0905 741.388 Q39.7508 744.03 46.212 744.03 Q52.6732 744.03 56.3653 741.388 Q60.0256 738.714 60.0256 734.067 Q60.0256 729.42 56.3653 726.779 Q52.6732 724.105 46.212 724.105 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M44.7161 677.826 L47.5806 677.826 L47.5806 704.753 Q53.6281 704.371 56.8109 701.125 Q59.9619 697.847 59.9619 692.022 Q59.9619 688.648 59.1344 685.497 Q58.3069 682.314 56.6518 679.195 L62.1899 679.195 Q63.5267 682.346 64.227 685.656 Q64.9272 688.966 64.9272 692.372 Q64.9272 700.902 59.9619 705.899 Q54.9967 710.864 46.5303 710.864 Q37.7774 710.864 32.6531 706.154 Q27.4968 701.411 27.4968 693.391 Q27.4968 686.197 32.1438 682.028 Q36.7589 677.826 44.7161 677.826 M42.9973 683.683 Q38.1912 683.747 35.3266 686.388 Q32.4621 688.998 32.4621 693.327 Q32.4621 698.229 35.2312 701.189 Q38.0002 704.117 43.0292 704.562 L42.9973 683.683 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M44.7161 637.722 L47.5806 637.722 L47.5806 664.649 Q53.6281 664.267 56.8109 661.021 Q59.9619 657.743 59.9619 651.918 Q59.9619 648.544 59.1344 645.393 Q58.3069 642.21 56.6518 639.091 L62.1899 639.091 Q63.5267 642.242 64.227 645.552 Q64.9272 648.862 64.9272 652.268 Q64.9272 660.798 59.9619 665.795 Q54.9967 670.76 46.5303 670.76 Q37.7774 670.76 32.6531 666.05 Q27.4968 661.307 27.4968 653.287 Q27.4968 646.093 32.1438 641.924 Q36.7589 637.722 44.7161 637.722 M42.9973 643.579 Q38.1912 643.643 35.3266 646.284 Q32.4621 648.894 32.4621 653.223 Q32.4621 658.125 35.2312 661.085 Q38.0002 664.013 43.0292 664.458 L42.9973 643.579 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M33.7671 604.653 L14.479 604.653 L14.479 598.796 L64.0042 598.796 L64.0042 604.653 L58.657 604.653 Q61.8398 606.499 63.3994 609.331 Q64.9272 612.132 64.9272 616.079 Q64.9272 622.54 59.771 626.614 Q54.6147 630.657 46.212 630.657 Q37.8093 630.657 32.6531 626.614 Q27.4968 622.54 27.4968 616.079 Q27.4968 612.132 29.0564 609.331 Q30.5842 606.499 33.7671 604.653 M46.212 624.609 Q52.6732 624.609 56.3653 621.967 Q60.0256 619.294 60.0256 614.647 Q60.0256 610 56.3653 607.326 Q52.6732 604.653 46.212 604.653 Q39.7508 604.653 36.0905 607.326 Q32.3984 610 32.3984 614.647 Q32.3984 619.294 36.0905 621.967 Q39.7508 624.609 46.212 624.609 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M49.9359 587.338 L28.3562 587.338 L28.3562 581.481 L49.7131 581.481 Q54.7739 581.481 57.3202 579.508 Q59.8346 577.535 59.8346 573.588 Q59.8346 568.846 56.8109 566.108 Q53.7872 563.339 48.5673 563.339 L28.3562 563.339 L28.3562 557.483 L64.0042 557.483 L64.0042 563.339 L58.5296 563.339 Q61.7762 565.472 63.3676 568.304 Q64.9272 571.105 64.9272 574.829 Q64.9272 580.972 61.1078 584.155 Q57.2883 587.338 49.9359 587.338 M27.4968 572.601 L27.4968 572.601 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M58.657 539.754 L77.5631 539.754 L77.5631 545.643 L28.3562 545.643 L28.3562 539.754 L33.7671 539.754 Q30.5842 537.908 29.0564 535.107 Q27.4968 532.275 27.4968 528.36 Q27.4968 521.867 32.6531 517.824 Q37.8093 513.75 46.212 513.75 Q54.6147 513.75 59.771 517.824 Q64.9272 521.867 64.9272 528.36 Q64.9272 532.275 63.3994 535.107 Q61.8398 537.908 58.657 539.754 M46.212 519.83 Q39.7508 519.83 36.0905 522.503 Q32.3984 525.145 32.3984 529.792 Q32.3984 534.439 36.0905 537.112 Q39.7508 539.754 46.212 539.754 Q52.6732 539.754 56.3653 537.112 Q60.0256 534.439 60.0256 529.792 Q60.0256 525.145 56.3653 522.503 Q52.6732 519.83 46.212 519.83 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip062)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"310.553,1372.3 429.468,1270.12 590.826,1222.53 1126.85,744.316 1501.51,557.133 1662.87,230.934 2037.53,158.811 2198.89,107.941 2293.27,85.838 \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M2020.1 196.379 L2282.7 196.379 L2282.7 92.6992 L2020.1 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2020.1,196.379 2282.7,196.379 2282.7,92.6992 2020.1,92.6992 2020.1,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2043.45,144.539 2183.57,144.539 \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M2220.76 164.227 Q2218.95 168.856 2217.24 170.268 Q2215.53 171.68 2212.66 171.68 L2209.26 171.68 L2209.26 168.115 L2211.76 168.115 Q2213.51 168.115 2214.49 167.282 Q2215.46 166.449 2216.64 163.347 L2217.4 161.403 L2206.92 135.893 L2211.43 135.893 L2219.53 156.171 L2227.63 135.893 L2232.15 135.893 L2220.76 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2239.44 157.884 L2247.08 157.884 L2247.08 131.518 L2238.77 133.185 L2238.77 128.926 L2247.03 127.259 L2251.71 127.259 L2251.71 157.884 L2259.35 157.884 L2259.35 161.819 L2239.44 161.819 L2239.44 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(mat_size,tcpu./tgpu, yaxis= :log, xaxis=:log, ylabel=\"GPU speedup\", xlabel=\"matrix size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2: Computing Custom Gradients on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider the challenge of moving from a CPU-array-based implementation to a GPU-based-implementation. In [this paper](https://arxiv.org/pdf/2406.13191), we needed to efficiently compute the gradient of the following expression:\n",
    "\n",
    "$\\mathcal{L}=-\\left\\Vert c^{T}+\\lambda^{T}A+\\mu^{T}C\\right\\Vert _{1}+\\lambda^{T}b+\\mu^{T}d.$\n",
    "\n",
    "with respect to the dual variables $\\lambda$ and $\\mu$. These gradients are given by\n",
    "* $\\frac{\\partial\\mathcal{L}}{\\partial\\lambda} =b-C{\\rm sign}\\left(c^{T}+\\lambda^{T}A+\\mu^{T}C\\right)$\n",
    "* $\\frac{\\partial\\mathcal{L}}{\\partial\\mu} =d-A{\\rm sign}\\left(c^{T}+\\lambda^{T}A+\\mu^{T}C\\right)$\n",
    "\n",
    "Highly optimzied, non-allocating CPU code looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_gradient (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function obj_gradient(lambda::Float64, mu::Vector{Float64}, grad_lambda::Float64, grad_mu::Vector{Float64}, A::Vector{Float64}, b::Vector{Float64}, C::Matrix{Float64}, d::Vector{Float64}, cg1::Vector{Float64}, ng::Int64, nl::Int64)\n",
    "    grad_lambda  = copy(b[1])\n",
    "    grad_mu     .= copy.(d)\n",
    "\n",
    "    for el in 1:ng\n",
    "        cv = @view C[:,el]\n",
    "        s_gamma = Float64(sign(cg1[el] + lambda*A[el] + dot(mu, cv)))\n",
    "\n",
    "        grad_lambda += -s_gamma*A[el]\n",
    "        for jj = 1:2*nl\n",
    "            grad_mu[jj] += -s_gamma*C[jj,el]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return grad_lambda, grad_mu\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this code with synthetic power system data, assuming 10,000 lines and 5,000 generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl          = 10000\n",
    "ng          = 5000\n",
    "lambda      = 0.0\n",
    "grad_lambda = 0.0\n",
    "mu          = randn(2*nl)\n",
    "grad_mu     = randn(2*nl)\n",
    "A           = randn(ng)\n",
    "b           = randn(1)\n",
    "C           = randn(2*nl,ng)\n",
    "d           = rand(2*nl)\n",
    "cg1         = rand(ng)\n",
    "\n",
    "cpu_gradient_time = @belapsed obj_gradient(lambda, mu, grad_lambda, grad_mu, A, b, C, d, cg1, ng, nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is **not efficient** with CuArrays! Why not? **Scalar indexing.** Generally, scalar indexing on GPUs is a bad idea. Recently, CUDA.jl has introduced ```UnifiedMemory```, where data accessed from the GPU device can be operated on via CPU with just ~2x latency, but this is for specialty applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\nIf you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\nto enable scalar iteration globally or for the operations in question.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore should be avoided.\n",
      "\n",
      "If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n",
      "to enable scalar iteration globally or for the operations in question.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base .\\error.jl:35\n",
      " [2] errorscalar(op::String)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:155\n",
      " [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:128\n",
      " [4] assertscalar(op::String)\n",
      "   @ GPUArraysCore C:\\Users\\chev8\\.julia\\packages\\GPUArraysCore\\GMsgk\\src\\GPUArraysCore.jl:116\n",
      " [5] getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:50 [inlined]\n",
      " [6] scalar_getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:36 [inlined]\n",
      " [7] _getindex\n",
      "   @ C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:19 [inlined]\n",
      " [8] getindex(::CuArray{Float64, 2, CUDA.DeviceMemory}, ::Int64, ::Int64)\n",
      "   @ GPUArrays C:\\Users\\chev8\\.julia\\packages\\GPUArrays\\qt4ax\\src\\host\\indexing.jl:17\n",
      " [9] top-level scope\n",
      "   @ c:\\Users\\chev8\\.julia\\dev\\ParallelNotes\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y104sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "x = CuArray(randn(10,10))\n",
    "x[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to make this code efficient, we had to reformulate the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_gradient_gpu_friendly (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function obj_gradient_gpu_friendly(lambda::Float64, mu::CuVector{Float64}, grad_lambda::Float64, grad_mu::CuVector{Float64}, a::CuVector{Float64}, b::Vector{Float64}, c::CuMatrix{Float64}, d::CuVector{Float64}, cg1::CuVector{Float64}, sign_gamma::CuVector{Float64}, ng::Int64, nl::Int64, ct::CuMatrix{Float64})\n",
    "    grad_lambda  = copy(b[1])\n",
    "    # the next two lines do this: sign_gamma .= sign.(cg1 .+ lambda.*a .+ c'*mu)\n",
    "    mul!(sign_gamma,ct,mu)\n",
    "    sign_gamma  .= sign.(cg1 .+ lambda.*a .+ sign_gamma)\n",
    "    grad_lambda -= dot(sign_gamma,a)\n",
    "    mul!(grad_mu,c,sign_gamma)\n",
    "    grad_mu .= .-grad_mu\n",
    "    grad_mu .+= d\n",
    "\n",
    "    return grad_lambda, grad_mu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0067429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl          = 10000\n",
    "ng          = 5000\n",
    "lambda      = 0.0\n",
    "grad_lambda = 0.0\n",
    "mu          = CuVector(randn(2*nl))\n",
    "grad_mu     = CuVector(randn(2*nl))\n",
    "A           = CuVector(randn(ng))\n",
    "b           = randn(1)\n",
    "C           = CuMatrix(randn(2*nl,ng))\n",
    "Ct          = copy(C')\n",
    "d           = CuVector(rand(2*nl))\n",
    "cg1         = CuVector(rand(ng))\n",
    "sign_gamma  = CuVector(zeros(ng))\n",
    "\n",
    "gpu_gradient_time = @belapsed CUDA.@sync obj_gradient_gpu_friendly(lambda, mu, grad_lambda, grad_mu, A, b, C, d, cg1, sign_gamma, ng, nl, Ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computation speedup: 13.33550846075131\n"
     ]
    }
   ],
   "source": [
    "println(\"Gradient computation speedup: \", cpu_gradient_time/gpu_gradient_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```The main lesson here is this: performant Julia code, which was written with scalar indexing to avoid memory allocation or for the sake of readability/understandability, will often need to be reformulated to run efficiently on the GPU.```\n",
    "* GPU arrays can be used to solve most of your problems, however, and should be used how typical abstract arrays are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, it is worth noting that CUDA has some awesome sparse, and non-sparse, linear system solver tools.\n",
    "* ```CUBLAS.getrf_batched!``` will **batch** LU factorize dense matrices on the GPU in parallel (with or without pivoting)\n",
    "* ```UBLAS.trsm_batched!``` will **batch** solve the resulting systems $Ly=b$; $Ux=y$\n",
    "* example application is in our recent [paper](https://arxiv.org/pdf/2311.11833)\n",
    "* Here is the LU solve function we wrote/used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lu_solve!(A::Array{CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, 1}, b::Array{CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, 1})\n",
    "    # solve Ax = LUx = b\n",
    "    # \n",
    "    # stage 1: solve \"Ly = b\"\n",
    "    side  = 'L'   # left side\n",
    "    ul    = 'L'   # solve \"L\"\n",
    "    tA    = 'N'   # not transposed\n",
    "    dA    = 'U'   # assume all ones on diagonal!\n",
    "    alpha = 1.0   # scalar\n",
    "    CUBLAS.trsm_batched!(side, ul, tA, dA, alpha, A, b);\n",
    "\n",
    "    # solve u\n",
    "    ul    = 'U'  # solve \"U\"\n",
    "    dA    = 'N'  # read diagonal\n",
    "    CUBLAS.trsm_batched!(side, ul, tA, dA, alpha, A, b);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. GPU Kernels ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While CuArray and the CUDA linear algebra libraries are very convenient and quite fast, custom GPU kernels can be used to achieve faster speeds still. A GPU kernel is \"a function that runs in parallel on a GPU\". It isn't like a custom function, however, since it returns \"nothing\"; instead, it mutates the given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```function!(x,y) -> nothing```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you launch your own Kernels, you get to **personally** manage the thread and block computations. Joy! This gives a high degree of computational granularity. Don't do this unless you are willing to invest the time in (1) writing good, safe code and (2) profiling against simpler methods to ensure there is actually a gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excellent overview of GPU threads/blocks is given [here](https://cuda.juliagpu.org/stable/tutorials/introduction/). Essentially, the kernel uses a **trick** to determing its own computational payload. In summary, this trick is based on these funcitons:\n",
    "1. ```threadIdx()``` calls the thread ID on a block\n",
    "2. ```blockDim()``` returns the dimension of the block\n",
    "3. ```blockIdx()``` returns the index of the block\n",
    "\n",
    "These expression can be queried in 3 dimensions (x,y,z). Blocks are managed and executed by a \"streaming multiprocessor\" (SM).\n",
    "\n",
    "![gpu](gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image comes from an overview of CUDA's [C/C++ library](https://developer.nvidia.com/blog/even-easier-introduction-cuda/). While Cu/GPU arrays are unique to the Julia API, block/thread management is a generic skill (useful in Julia or C/C++)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: when working with GPU kernels, scalar indexing (calling ```v[i]```) ~~is really bad~~ **is good and necessary!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: Simple Cuda Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $m$ quadratic systems, each of whose energy we can compute individually as $e_{i}=x_{i}^{T}A_{i}x_{i}$. We can compute these in parallel using GPU kernels. On the cpu, the code is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_cpu! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_cpu!(A,x,e)\n",
    "    for i in 1:length(e)\n",
    "        e[i] = x[i]'*A[i]*x[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "A = [randn(n_states,n_states) for ii in 1:n_particles]\n",
    "x = [randn(n_states) for ii in 1:n_particles]\n",
    "e = [randn() for ii in 1:n_particles]\n",
    "\n",
    "t_cpu = @belapsed quadratic_computation_cpu!(A,x,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a gpu kernel to accomplish the same task. Note: the kernel is launched by calling ```@cuda``` and then passing the optional keywords \n",
    "* ```threads```, to specify the number of threads that should launch, and\n",
    "* ```blocks```, to specify the number of blocks\n",
    "\n",
    "First, we define a kernel function. Note:\n",
    "* it returns nothing\n",
    "* it directly mutates the inputs\n",
    "* it uses scalar indexing to access devise memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu!(A,x,e,n_states)\n",
    "    i = threadIdx().x\n",
    "    for jj in 1:n_states # note -- row major!\n",
    "        for ii in 1:n_states\n",
    "            @inbounds e[i] += sum(x[ii,i]*A[ii,jj,i]*x[jj,i])\n",
    "        end\n",
    "    end\n",
    "    return # returns nothing :)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **trick** is this: the tiny little thread only knows two things: its name (i.e., its index), and how to do some very basic math. It uses its name (i.e., its numerical index) to decide which math it should do.\n",
    "\n",
    "Let's specify the number of threads we want to use, and then call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Number of threads in x-dimension exceeds device limit (40960 > 1024).",
     "output_type": "error",
     "traceback": [
      "Number of threads in x-dimension exceeds device limit (40960 > 1024).\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base .\\error.jl:35\n",
      "  [2] diagnose_launch_failure(f::CuFunction, err::CuError; blockdim::CuDim3, threaddim::CuDim3, shmem::Int64)\n",
      "    @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:97\n",
      "  [3] launch(::CuFunction, ::CUDA.KernelState, ::CuDeviceArray{Float64, 3, 1}, ::CuDeviceMatrix{Float64, 1}, ::CuDeviceVector{Float64, 1}, ::Int64; blocks::Int64, threads::Int64, cooperative::Bool, shmem::Int64, stream::CuStream)\n",
      "    @ CUDA C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:73\n",
      "  [4] launch\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:52 [inlined]\n",
      "  [5] #972\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:189 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:149 [inlined]\n",
      "  [7] macro expansion\n",
      "    @ .\\none:0 [inlined]\n",
      "  [8] convert_arguments\n",
      "    @ .\\none:0 [inlined]\n",
      "  [9] #cudacall#971\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:191 [inlined]\n",
      " [10] cudacall\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\lib\\cudadrv\\execution.jl:187 [inlined]\n",
      " [11] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\compiler\\execution.jl:279 [inlined]\n",
      " [12] macro expansion\n",
      "    @ .\\none:0 [inlined]\n",
      " [13] #_#1157\n",
      "    @ .\\none:0 [inlined]\n",
      " [14] macro expansion\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\CUDA\\2kjXI\\src\\compiler\\execution.jl:114 [inlined]\n",
      " [15] var\"##core#363\"()\n",
      "    @ Main C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:561\n",
      " [16] var\"##sample#364\"(::Tuple{}, __params::BenchmarkTools.Parameters)\n",
      "    @ Main C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:570\n",
      " [17] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; maxevals::Int64, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:187\n",
      " [18] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters)\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:182\n",
      " [19] #invokelatest#2\n",
      "    @ .\\essentials.jl:1055 [inlined]\n",
      " [20] invokelatest\n",
      "    @ .\\essentials.jl:1052 [inlined]\n",
      " [21] #lineartrial#46\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:51 [inlined]\n",
      " [22] lineartrial\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:50 [inlined]\n",
      " [23] tune!(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, verbose::Bool, pad::String, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:300\n",
      " [24] tune!\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:289 [inlined]\n",
      " [25] tune!(b::BenchmarkTools.Benchmark)\n",
      "    @ BenchmarkTools C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:289\n",
      " [26] top-level scope\n",
      "    @ C:\\Users\\chev8\\.julia\\packages\\BenchmarkTools\\QNsku\\src\\execution.jl:447"
     ]
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "@belapsed @cuda threads=length(e) quadratic_computation_gpu!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OH NOOOO!** We tried to launch too many threads -- we only have 1024 on my GPU. We could either\n",
    "1. 40x the amout of work each thread has to do, or\n",
    "2. ask for additional blocks\n",
    "\n",
    "Let's write both. Here is an updated function for option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu_overload! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu_overload!(A,x,e,n_states)\n",
    "    i = threadIdx().x\n",
    "    for jj in 1:n_states # note -- row major!\n",
    "        for ii in 1:n_states\n",
    "            for kk in 0:39\n",
    "                @inbounds e[i + kk*1024] += sum(x[ii,i + kk*1024]*A[ii,jj,i + kk*1024]*x[jj,i + kk*1024])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0069055"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "@belapsed CUDA.@sync @cuda threads=1024 quadratic_computation_gpu_overload!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to good -- we didn't exploit parallelism very effectively, because we overloaded those poor threads.\n",
    "\n",
    "Here is how we can apply option 2, where we deploy blocks with additional threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadratic_computation_gpu_blocks! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function quadratic_computation_gpu_blocks!(A,x,e,n_states)\n",
    "    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    # if needed, we can also define a stride to loop over larger vectors -- not used here\n",
    "    # stride = gridDim().x * blockDim().x\n",
    "\n",
    "    @inbounds for ii in 1:n_states # note -- row major!\n",
    "        @inbounds for jj in 1:n_states\n",
    "            @inbounds e[index] += x[ii,index]*A[ii,jj,index]*x[jj,index]\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_particles = 40*1024\n",
    "n_states    = 10\n",
    "numblocks   = 40\n",
    "A = CuArray(randn(n_states,n_states,n_particles))\n",
    "x = CuArray(randn(n_states,n_particles))\n",
    "e = CuArray(randn(n_particles))\n",
    "\n",
    "t_gpu = @belapsed CUDA.@sync @cuda threads=1024 blocks=numblocks quadratic_computation_gpu_blocks!(A,x,e,n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are at least 50% faster than the CPU, but we can be much faster still if we exploit multi-dimensional multithreading (x,y,z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-CUDA kernels\n",
    "For non-CUDA kernels, Julia has [KernelAbstractions.jl](https://juliagpu.github.io/KernelAbstractions.jl/stable/examples/matmul/). Here is an [excellent video](https://www.youtube.com/watch?v=1Q5Hpwu3tpo&t=3590s), too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2: An ADMM example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of us are familiar with ADMM, a distributed optimization technique which iteratively minimizes a set of distributed primal objectives, takes a dual ascent step, and continues on. In a [recent paper](https://arxiv.org/pdf/2310.09410) by Ryu et al., they solve the following problem over $|S|$ subsystems:\n",
    "\n",
    "![s](.\\figs\\admm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They forumularte a Lagrange dual, and then apply stationarity conditions to explictly compute a solution for the primal as a function of the duals:\n",
    "\n",
    "![s](.\\figs\\admm_soln.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update algorithm is given below:\n",
    "![s](.\\figs\\alg.png)\n",
    "Given the following primal and dual update rules, **can you write a kernel?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here is the CPU code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS     = 100 # number of systems\n",
    "nv     = 50  # number of variables\n",
    "C      = randn(nv)\n",
    "x      = [randn(nv)    for S in 1:nS]\n",
    "lambda = [randn(nv)    for S in 1:nS]\n",
    "xhat   = [randn(nv)    for S in 1:nS]\n",
    "B      = [randn(nv,nv) for S in 1:nS]\n",
    "xupper = [10*rand(nv)  for S in 1:nS]\n",
    "xlower = [-10*rand(nv) for S in 1:nS]\n",
    "\n",
    "for S in 1:nS\n",
    "\n",
    "    # x update\n",
    "    xhat[S] = (-1)*(C + sum(B[S]'*lambda[S] for S in 1:nS) - sum(x')')\n",
    "    x[S] = min.( max.(xhat[S], xlower[S]), xupper[S])\n",
    "    \n",
    "    # update lambda\n",
    "    lambda[S] = lambda[S] + p*(B[S]*x[S]-x[S])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function admm_kernel(C, x, xhat, lambda, xupper, xlower)\n",
    "\n",
    "    return: nothing\n",
    "end\n",
    "\n",
    "# call the kernel!\n",
    "\n",
    "@cude threads blocks admm_kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
