{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unexpected-blink",
   "metadata": {},
   "source": [
    "# Neural Networks Overview\n",
    "Alyssa Kody  \n",
    "March 9, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-powell",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- A **neuron** maps a vector input $x$ to output $y$ as:   $\\quad y = f(wx + b)$\n",
    "\n",
    "- The neuron performs three operations on the inputs in the following order:\n",
    "    1. Scale: $w$ is a weighting matrix\n",
    "    2. Add: $b$ is bias\n",
    "    3. Activation function: $f(\\cdot)$\n",
    "    \n",
    "    \n",
    "- The **activation function** introduces non-linearity into the network, otherwise, summing and scaling linear functions would just produce another linear function, and most likely, you would like to model *nonlinear systems*\n",
    "\n",
    "- Example activation functions:\n",
    "    1. Sigmoid: $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "    2. ReLU (rectified linear unit): \n",
    "     $f(x) = \\begin{cases}\n",
    "    0, & x<0 \\\\ \n",
    "    x, & x \\geq 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-cement",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "- Layering neurons together creates a **neural network** \n",
    "\n",
    "- Multiple Layers of neurons is a **deep neural network**\n",
    "\n",
    "<img src=\"neural_network.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Three common types of neural networks:\n",
    "1. **Feedforward neural network**\n",
    "    - Simplest type of neural network.\n",
    "    - Information only moves one direction within the network.\n",
    "    \n",
    "    \n",
    "2. **Recurrent neural network (RNN)**\n",
    "    - Commonly used for language processing.\n",
    "    \n",
    "    \n",
    "3. **Convolution neural network (CNN)**\n",
    "    - Commonly used for image classification.\n",
    "    - Works well for large input sets where the is a relationship between neighboring inputs (like pixels in an image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-harrison",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "- We want to tune the neuron parameters $w_1, w_2, \\ldots, w_N$ and $b_1, b_2, \\ldots, b_N$\n",
    "\n",
    "- In order to train a neural network, we need to first identify a function that measures how good our neural network is doing: a **loss function**.\n",
    "\n",
    "- The loss function is ultimately used to calculate gradients, and then we use these gradients to update the weights and bias.\n",
    "\n",
    "- Broadly, we can group loss functions in two categories:\n",
    "    1. **Regression losses:** Typically used when the output is a real-valued number\n",
    "        - ex. *Mean Squared Error:* $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_{true} - y_{pred})^2$, most commonly used loss function for regression tasks.\n",
    "    2. **Probabilistic losses:** Used for predictive modeling problems where inputs are assigned to labels.\n",
    "        - ex. *Binary Crossentropy (Log Loss):* used when you have one output node to classify the data into two classes..\n",
    "        - ex. *Categorical Crossentropy:* used for multi-class classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-louisiana",
   "metadata": {},
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "- Our goal is to minimize the loss by altering the network's weights and biases.\n",
    "\n",
    "- Let $L(w,b)$ be the loss function, which is a function of the weights $w$ and the biases $b$.\n",
    "\n",
    "- We will use **stochastic gradient descent (SGD)**\n",
    "    - Iterative optimization method\n",
    "    - Performs a parameter update for each training sample\n",
    "\n",
    "\n",
    "- The algorithm sweeps through data samples (in this case, our training data), and performs an update for each parameter:\n",
    "    $$w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i} \\qquad \\text{and} \\qquad b_i \\leftarrow b_i - \\eta \\frac{\\partial L}{\\partial b_i}$$\n",
    "where $\\eta$ is the *learning rate* and we can find the partial derivatives $\\frac{\\partial L}{\\partial w_i}$ and $\\frac{\\partial L}{\\partial b_i}$ by doing the following:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial L}{\\partial w_i} =& \\left(\\frac{\\partial L}{\\partial y_{pred}}\\right) \\left(\\frac{\\partial y_{pred}}{\\partial h_i} \\right) \\left(\\frac{\\partial h_{i}}{\\partial w_i} \\right)\\\\\n",
    "    \\frac{\\partial L}{\\partial b_i} =& \\left(\\frac{\\partial L}{\\partial y_{pred}}\\right) \\left(\\frac{\\partial y_{pred}}{\\partial b_i} \\right) \\left(\\frac{\\partial b_{i}}{\\partial b_i} \\right)\n",
    "\\end{align}\n",
    "where $h_i$ is the $i^{th}$ neuron.\n",
    "- This is called **backpropagation** (also just literally the chain rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-employee",
   "metadata": {},
   "source": [
    "# Machine Learning Software Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-wyoming",
   "metadata": {},
   "source": [
    "- Common Machine Learning Libraries:\n",
    "    1. **TensorFlow** (Google, Python)\n",
    "    2. **PyTorch** (Facebook, Python)\n",
    "    3. **Flux** (Not associated with a company that I know of, Julia)\n",
    "\n",
    "\n",
    "- Types of differentiation:\n",
    "    1. Symbolic differentiation\n",
    "        - Calculus class, Wolfram Alpha, Mathematica\n",
    "        - Slow\n",
    "    2. Numerical differentiation\n",
    "        - e.g., trapz in matlab\n",
    "        - Can be inaccurate\n",
    "    3. Automatic differentiation (autodiff or AD)\n",
    "        - Repeatedly executes the chain rule for a sequence of elementary arithmetic operations/functions (e.g., $+$, $-$, $\\times$, $\\div$, exp, log, sin, cos)\n",
    "        - Fast and can handle many inputs\n",
    "        - Flux uses **Zygote.jl**\n",
    "        \n",
    "        \n",
    "- What about activation functions that are not differentiable at all possible input variables?\n",
    "    - E.g., ReLU (rectified linear unit): \n",
    "         $f(x) = \\begin{cases}\n",
    "        0, & x<0 \\\\ \n",
    "        x, & x \\geq 0\n",
    "        \\end{cases}$\n",
    "    - Not differentiable at $x=0$\n",
    "    - Software (including Zygote) usually returns one of the one-sided derivatives instead of an error.\n",
    "    -  Very unlikely that $x$ is ever *identically* equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-breath",
   "metadata": {},
   "source": [
    "# Flux Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the project\n",
    "cd(\"/home/akody/Flux_Tutorial\")\n",
    "\n",
    "# Set environment\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "using Flux, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e910a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "for ii in 1:100\n",
    "    println(ii)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-shannon",
   "metadata": {},
   "source": [
    "### Modeling a single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-expansion",
   "metadata": {},
   "source": [
    "Create layer with 2 inputs, 1 output and a sigmoid acitivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-northern",
   "metadata": {},
   "source": [
    "There are built-in activation functions, or you can write a custom activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(σ, label=false)\n",
    "savefig(p1, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = plot(relu, label=false)\n",
    "savefig(p2, \"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-spending",
   "metadata": {},
   "source": [
    "Access the biases and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = σ.(model.W*x + model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-guess",
   "metadata": {},
   "source": [
    "Often we do not need the precision of a Float64, and using a Float32 uses half the memory, resulting in a speed up.  \n",
    "Also note that the inputs and outputs of \"model\" are always arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-conclusion",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-georgia",
   "metadata": {},
   "source": [
    "Built in loss functions in Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Flux.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y_true) = Flux.mse(model(x),y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = rand(1)\n",
    "loss(x, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-verse",
   "metadata": {},
   "source": [
    "### Taking gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 5x^2 + 6x + 16\n",
    "df(x) = gradient(f, x)[1]\n",
    "df2(x) = gradient(df, x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-entertainment",
   "metadata": {},
   "source": [
    "What about functions that aren't differentiable at all points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = relu(x)\n",
    "dg(x) = gradient(g, x)[1]\n",
    "dg(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-commitment",
   "metadata": {},
   "source": [
    "Can access the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Flux.params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-iceland",
   "metadata": {},
   "source": [
    "These are special types of arrays. Recall Zygote is the autodiff package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grads = gradient(() -> loss(x,y_true), Flux.params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grads[Flux.params(model)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grads[Flux.params(model)[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_dyhat = 2*(model(x) .- y_true)\n",
    "dy_hat_dW = σ.(model.W*x + model.b)[1]*(1-σ.(model.W*x + model.b)[1])*x\n",
    "dloss_dW = dloss_dyhat.*dy_hat_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_dyhat = 2*(model(x) .- y_true)\n",
    "dy_hat_db = σ.(model.W*x + model.b)[1]*(1-σ.(model.W*x + model.b)[1])\n",
    "dloss_db = dloss_dyhat.*dy_hat_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-documentary",
   "metadata": {},
   "source": [
    "### Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-camcorder",
   "metadata": {},
   "source": [
    "Gradient descent optimiser with learning rate $\\eta$.\n",
    "For each parameter $p$ and its gradient $\\delta$,\n",
    "\n",
    "$$p \\leftarrow p - \\eta \\delta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Descent(0.5)\n",
    "param_grads = gradient(() -> loss(x,y_true), Flux.params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.update!(opt, Flux.params(model), param_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-glasgow",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-clinic",
   "metadata": {},
   "source": [
    "Take gradients and update parameters using Flux.train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = rand(2, 100), fill(0.5, 1, 100)\n",
    "\n",
    "Flux.train!(loss, params(model), [(data,labels)], opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-bacteria",
   "metadata": {},
   "source": [
    "### Model neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-illness",
   "metadata": {},
   "source": [
    "Neural network with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Chain(\n",
    "    Dense(2,5,relu), \n",
    "    Dense(5,5,relu), \n",
    "    Dense(5,1,σ)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-assistant",
   "metadata": {},
   "source": [
    "### Simple Single Generator Infinite Bus Stability Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-cocktail",
   "metadata": {},
   "source": [
    "The Single Machine Infinite Bus (SMIB) System is modeled as: $\\quad m_1 \\ddot{\\delta} + d_1\\dot{\\delta} + B_{12} V_1 V_2 \\sin{(\\delta)} - P_1 = 0$\n",
    "\n",
    "This is equivalent to a pendulum:\n",
    "\n",
    "<img src=\"pendulum.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "There is threshold for $|P_1|$, above which, the system becomes unstable.  \n",
    "Although we know we can easily find this threshold analytically, let's try to find it using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "\n",
    "#model parameters\n",
    "V1 = 1\n",
    "V2 = 1\n",
    "B = 0.2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data and labels\n",
    "num_training_samples = 1000\n",
    "num_test_samples = 1000\n",
    "\n",
    "#sample \n",
    "P_critial = B*V1*V2\n",
    "P_min = 0\n",
    "P_max = P_critial*2\n",
    "train_P = rand(Uniform(P_min,P_max), 1, num_training_samples)\n",
    "test_P = rand(Uniform(P_min,P_max), 1, num_test_samples)\n",
    "\n",
    "#create the training labels\n",
    "#1==stable, 0==unstable\n",
    "train_labels = (train_P .< P_critial).*1\n",
    "test_labels = (test_P .< P_critial).*1\n",
    "num_stable_train = sum(train_labels)\n",
    "num_stable_test = sum(test_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data\n",
    "train_P_scaled = train_P.*(1/P_max)\n",
    "test_P_scaled = test_P.*(1/P_max)\n",
    "\n",
    "#Build the model.\n",
    "SMIB_model = Chain(\n",
    "    Dense(1, 4, σ),\n",
    "    Dense(4, 4, σ),\n",
    "    Dense(4, 1, σ)\n",
    ")\n",
    "\n",
    "#Identify the loss function\n",
    "loss(x,y) = Flux.Losses.binarycrossentropy(SMIB_model(x), y)\n",
    "\n",
    "#Set the optimization\n",
    "opt = Descent(1)\n",
    "\n",
    "starting_loss = loss(train_P_scaled, train_labels)\n",
    "println(\"starting loss: $starting_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "loss_history = []\n",
    "\n",
    "epochs = 5000\n",
    "for i in 1:epochs\n",
    "    Flux.train!(loss, Flux.params(SMIB_model), [(train_P_scaled, train_labels)], opt)\n",
    "    push!(loss_history, loss(train_P_scaled, train_labels))\n",
    "end\n",
    "\n",
    "end_loss = loss(train_P_scaled, train_labels)\n",
    "println(\"end loss: $end_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(loss_history, label=false, xlabel = \"epoch\", ylabel = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = SMIB_model(train_P_scaled)\n",
    "\n",
    "index_sorted = sortperm(vec(train_P_scaled))\n",
    "train_labels_sorted = train_labels[index_sorted]\n",
    "predictions_train_sorted = predictions_train[index_sorted]\n",
    "train_P_sorted = train_P[index_sorted]\n",
    "\n",
    "p1 = plot(train_P_sorted, train_labels_sorted, label = \"true\")\n",
    "plot!(train_P_sorted, predictions_train_sorted, label = \"predicted\")\n",
    "plot!(xlabel = \"P\", ylabel = \"label\", title = \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = SMIB_model(test_P_scaled)\n",
    "\n",
    "index_sorted = sortperm(vec(test_P_scaled))\n",
    "test_labels_sorted = test_labels[index_sorted]\n",
    "predictions_test_sorted = predictions_test[index_sorted]\n",
    "test_P_sorted = test_P[index_sorted]\n",
    "\n",
    "p1 = plot(test_P_sorted, test_labels_sorted, label = \"true\")\n",
    "plot!(test_P_sorted, predictions_test_sorted, label = \"predicted\")\n",
    "plot!(xlabel = \"P\", ylabel = \"label\", title = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(0.5, digits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sorted_rounded = round.(vec(predictions_test_sorted), digits=0)\n",
    "correct_indexes = (predictions_sorted_rounded.==test_labels_sorted).*1\n",
    "incorrect_indexes = 1 .- correct_indexes\n",
    "\n",
    "p3 = scatter(test_P_sorted[findall(correct_indexes.==1)], predictions_test_sorted[findall(correct_indexes.==1)], label = \"correct\")\n",
    "scatter!(test_P_sorted[findall(incorrect_indexes.==1)], predictions_test_sorted[findall(incorrect_indexes.==1)], label = \"incorrect\")\n",
    "plot!(xlabel = \"P\", ylabel = \"prediction\" )\n",
    "display(p3)\n",
    "\n",
    "percent_correct = sum(correct_indexes)/length(correct_indexes)\n",
    "println(\"percent_correct = $percent_correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-injury",
   "metadata": {},
   "source": [
    "# Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-mauritius",
   "metadata": {},
   "source": [
    "Resources I found useful to learn Flux:\n",
    "\n",
    "1. Flux Webpage: https://fluxml.ai/\n",
    "2. Basics: https://fluxml.ai/Flux.jl/stable/models/basics/\n",
    "3. Training: https://fluxml.ai/Flux.jl/stable/training/training/\n",
    "4. 60-min Tutorial: https://fluxml.ai/tutorials/2020/09/15/deep-learning-flux.html\n",
    "5. Julia Academy Short Course: https://juliaacademy.com/p/deep-learning-with-flux-jl\n",
    "6. Check out other people's code on Github!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-check",
   "metadata": {},
   "source": [
    "Other useful packages:\n",
    "https://fluxml.ai/Flux.jl/stable/ecosystem/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
